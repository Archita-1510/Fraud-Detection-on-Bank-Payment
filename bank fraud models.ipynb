{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bank_fraud models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "nFFFKQslj_rO",
        "outputId": "4d69511f-905c-446c-e237-234dd7a0aa0e"
      },
      "source": [
        "from google.colab import files\n",
        "upload=files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3e33cf32-a2c8-440a-be53-460a46fa51ee\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3e33cf32-a2c8-440a-be53-460a46fa51ee\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving fraud payment data.csv to fraud payment data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IId2t79TkKX4",
        "outputId": "83a8be39-414d-4525-e406-876e17f3166f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import (SMOTE, RandomOverSampler)\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, recall_score, precision_score, accuracy_score, f1_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "import io\n",
        "df = pd.read_csv(io.BytesIO(upload['fraud payment data.csv']),encoding='latin-1')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "sElO1Q2dkWGs",
        "outputId": "657c12d0-c7f5-4b8a-ae7a-b4d105c5082f"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>step</th>\n",
              "      <th>customer</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>zipcodeOri</th>\n",
              "      <th>merchant</th>\n",
              "      <th>zipMerchant</th>\n",
              "      <th>category</th>\n",
              "      <th>amount</th>\n",
              "      <th>fraud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>'C1093826151'</td>\n",
              "      <td>'4'</td>\n",
              "      <td>'M'</td>\n",
              "      <td>'28007'</td>\n",
              "      <td>'M348934600'</td>\n",
              "      <td>'28007'</td>\n",
              "      <td>'es_transportation'</td>\n",
              "      <td>4.55</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>'C352968107'</td>\n",
              "      <td>'2'</td>\n",
              "      <td>'M'</td>\n",
              "      <td>'28007'</td>\n",
              "      <td>'M348934600'</td>\n",
              "      <td>'28007'</td>\n",
              "      <td>'es_transportation'</td>\n",
              "      <td>39.68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>'C2054744914'</td>\n",
              "      <td>'4'</td>\n",
              "      <td>'F'</td>\n",
              "      <td>'28007'</td>\n",
              "      <td>'M1823072687'</td>\n",
              "      <td>'28007'</td>\n",
              "      <td>'es_transportation'</td>\n",
              "      <td>26.89</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>'C1760612790'</td>\n",
              "      <td>'3'</td>\n",
              "      <td>'M'</td>\n",
              "      <td>'28007'</td>\n",
              "      <td>'M348934600'</td>\n",
              "      <td>'28007'</td>\n",
              "      <td>'es_transportation'</td>\n",
              "      <td>17.25</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>'C757503768'</td>\n",
              "      <td>'5'</td>\n",
              "      <td>'M'</td>\n",
              "      <td>'28007'</td>\n",
              "      <td>'M348934600'</td>\n",
              "      <td>'28007'</td>\n",
              "      <td>'es_transportation'</td>\n",
              "      <td>35.72</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   step       customer  age  ...             category amount fraud\n",
              "0     0  'C1093826151'  '4'  ...  'es_transportation'   4.55     0\n",
              "1     0   'C352968107'  '2'  ...  'es_transportation'  39.68     0\n",
              "2     0  'C2054744914'  '4'  ...  'es_transportation'  26.89     0\n",
              "3     0  'C1760612790'  '3'  ...  'es_transportation'  17.25     0\n",
              "4     0   'C757503768'  '5'  ...  'es_transportation'  35.72     0\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2dfJwMLklBG"
      },
      "source": [
        "modifying the data frame according to our needs\n",
        "1. changing the categorical variables to numerical type for the ease of computation\n",
        "2. dropping the columns with merchant and customer zip code because both the columns take only 1 value for the entire data set\n",
        "3. dropping transactions with amount=0, because if there has been no transaction at all there is no question of fraud in that case, having such data might be misleading and increases the non fraud cases unnecessarilly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "_sPoqQdalEbp",
        "outputId": "e87a0c70-01ea-43d2-a6ab-171e6daf92a0"
      },
      "source": [
        "encoder=LabelEncoder()\n",
        "df['customer']=encoder.fit_transform(df['customer'])\n",
        "df['age']=encoder.fit_transform(df['age'])\n",
        "df['gender']=encoder.fit_transform(df['gender'])\n",
        "df['merchant']=encoder.fit_transform(df['merchant'])\n",
        "df['category']=encoder.fit_transform(df['category'])\n",
        "df.drop(['zipcodeOri','zipMerchant'],axis=1,inplace=True)\n",
        "df=df.query('amount>0')\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>step</th>\n",
              "      <th>customer</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>merchant</th>\n",
              "      <th>category</th>\n",
              "      <th>amount</th>\n",
              "      <th>fraud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>210</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>30</td>\n",
              "      <td>12</td>\n",
              "      <td>4.55</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2753</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>30</td>\n",
              "      <td>12</td>\n",
              "      <td>39.68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2285</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>12</td>\n",
              "      <td>26.89</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1650</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>30</td>\n",
              "      <td>12</td>\n",
              "      <td>17.25</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3585</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>30</td>\n",
              "      <td>12</td>\n",
              "      <td>35.72</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   step  customer  age  gender  merchant  category  amount  fraud\n",
              "0     0       210    4       2        30        12    4.55      0\n",
              "1     0      2753    2       2        30        12   39.68      0\n",
              "2     0      2285    4       1        18        12   26.89      0\n",
              "3     0      1650    3       2        30        12   17.25      0\n",
              "4     0      3585    5       2        30        12   35.72      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uImjUSsgrQrJ"
      },
      "source": [
        "splitting the data frame into 2 other data frame, one for fraud and one for non fraud cases respectively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL2RvpVwrJwc",
        "outputId": "ec151c68-141b-46a6-83d0-16ec87b04f93"
      },
      "source": [
        "fraud = df[df['fraud']==1]\n",
        "non_fraud = df[df['fraud']==0]\n",
        "print(fraud.shape , non_fraud.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7200, 8) (587391, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVIjeTV1lXBq"
      },
      "source": [
        "splitting the data from into dependent (y) and independent variables (x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93wHoO1xld0N",
        "outputId": "727fb588-9f75-4f7a-ec22-875e467e2d09"
      },
      "source": [
        "x=df.drop('fraud',axis=1)\n",
        "y=df['fraud']\n",
        "print(x.head())\n",
        "print(y.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   step  customer  age  gender  merchant  category  amount\n",
            "0     0       210    4       2        30        12    4.55\n",
            "1     0      2753    2       2        30        12   39.68\n",
            "2     0      2285    4       1        18        12   26.89\n",
            "3     0      1650    3       2        30        12   17.25\n",
            "4     0      3585    5       2        30        12   35.72\n",
            "0    0\n",
            "1    0\n",
            "2    0\n",
            "3    0\n",
            "4    0\n",
            "Name: fraud, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23dTx2Mflllf"
      },
      "source": [
        "correlation between the variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "bK4V0QEpltTi",
        "outputId": "4e576376-d731-4ec1-f6e6-2002184a509e"
      },
      "source": [
        "plt.subplots(figsize=(10,10))\n",
        "sns.heatmap(df.corr(),annot=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb02e3f8790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqf0R-fzl3wt"
      },
      "source": [
        "checking whether fraud is dependent on other categorical variables or not, using chi square test of independence for categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyrG_G99mDih",
        "outputId": "57c0a898-3643-4831-c772-551dae09f333"
      },
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "chisq=df\n",
        "alpha=0.05\n",
        "\n",
        "print('chi sq test of independence between fraud and steps')\n",
        "chisq_table=pd.crosstab(chisq['fraud'],chisq['step'])\n",
        "stat, p, dof, expected = chi2_contingency(chisq_table)\n",
        "# interpret p-value\n",
        "print(\"p value is \" + str(p))\n",
        "if p <= alpha:\n",
        "     print('Dependent (reject H0)')\n",
        "else:\n",
        "     print('Independent (H0 holds true)')\n",
        "print('\\n')\n",
        "\n",
        "print('chi sq test of independence between fraud and customer')\n",
        "chisq_table=pd.crosstab(chisq['fraud'],chisq['customer'])\n",
        "stat, p, dof, expected = chi2_contingency(chisq_table)\n",
        "# interpret p-value\n",
        "print(\"p value is \" + str(p))\n",
        "if p <= alpha:\n",
        "     print('Dependent (reject H0)')\n",
        "else:\n",
        "     print('Independent (H0 holds true)')\n",
        "print('\\n')\n",
        "\n",
        "print('chi sq test of independence between fraud age')\n",
        "chisq_table=pd.crosstab(chisq['fraud'],chisq['age'])\n",
        "stat, p, dof, expected = chi2_contingency(chisq_table)\n",
        "# interpret p-value\n",
        "print(\"p value is \" + str(p))\n",
        "if p <= alpha:\n",
        "     print('Dependent (reject H0)')\n",
        "else:\n",
        "     print('Independent (H0 holds true)')\n",
        "print('\\n')\n",
        "\n",
        "print('chi sq test of independence between fraud and gender')\n",
        "chisq_table=pd.crosstab(chisq['fraud'],chisq['gender'])\n",
        "stat, p, dof, expected = chi2_contingency(chisq_table)\n",
        "# interpret p-value\n",
        "print(\"p value is \" + str(p))\n",
        "if p <= alpha:\n",
        "     print('Dependent (reject H0)')\n",
        "else:\n",
        "     print('Independent (H0 holds true)')\n",
        "print('\\n')\n",
        "\n",
        "print('chi sq test of independence between fraud and merchant')\n",
        "chisq_table=pd.crosstab(chisq['fraud'],chisq['merchant'])\n",
        "stat, p, dof, expected = chi2_contingency(chisq_table)\n",
        "# interpret p-value\n",
        "print(\"p value is \" + str(p))\n",
        "if p <= alpha:\n",
        "     print('Dependent (reject H0)')\n",
        "else:\n",
        "     print('Independent (H0 holds true)')\n",
        "print('\\n')\n",
        "\n",
        "print('chi sq test of independence between fraud and category')\n",
        "chisq_table=pd.crosstab(chisq['fraud'],chisq['category'])\n",
        "stat, p, dof, expected = chi2_contingency(chisq_table)\n",
        "# interpret p-value\n",
        "print(\"p value is \" + str(p))\n",
        "if p <= alpha:\n",
        "     print('Dependent (reject H0)')\n",
        "else:\n",
        "     print('Independent (H0 holds true)')\n",
        "print('\\n')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chi sq test of independence between fraud and steps\n",
            "p value is 0.999999936271118\n",
            "Independent (H0 holds true)\n",
            "\n",
            "\n",
            "chi sq test of independence between fraud and customer\n",
            "p value is 0.0\n",
            "Dependent (reject H0)\n",
            "\n",
            "\n",
            "chi sq test of independence between fraud age\n",
            "p value is 2.00866504430546e-07\n",
            "Dependent (reject H0)\n",
            "\n",
            "\n",
            "chi sq test of independence between fraud and gender\n",
            "p value is 5.932305999855071e-85\n",
            "Dependent (reject H0)\n",
            "\n",
            "\n",
            "chi sq test of independence between fraud and merchant\n",
            "p value is 0.0\n",
            "Dependent (reject H0)\n",
            "\n",
            "\n",
            "chi sq test of independence between fraud and category\n",
            "p value is 0.0\n",
            "Dependent (reject H0)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovbEDy98mbQk"
      },
      "source": [
        "splitting into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jYb4sR_l2ap"
      },
      "source": [
        "x_new=pd.DataFrame(data=df,columns=['amount','category','merchant','gender','age','customer'])\n",
        "x_train,x_test,y_train,y_test=train_test_split(x_new,y,test_size=0.2,random_state=0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRkNbLNXmiek"
      },
      "source": [
        "creating a new data frame for the ease of keeping track of how each models work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "nq2ClW3Glh3G",
        "outputId": "4a549e16-c81c-450e-97a2-9a4ab3b11a8f"
      },
      "source": [
        "models={'name':['logistic regression (simple)','logistic regression (under sampling)','logistic regression (over sampling)','logistic regression (smote)','support vector machine (simple)','support vector machine (under sampling)','support vector machine (over sampling)','support vector machine (smote)','decision tree (simple)','decision tree (under sampling)','decision tree (over sampling)','decision tree (smote)','random forest (simple)','random forest (under sampling)','random forest (over sampling)','random forest (smote)','naive bayes (simple)','naive bayes (under sampling)','naive bayes (over sampling)','naive bayes (smote)','knn (simple)','knn (under sampling)','knn (over sampling)','knn (smote)']}\n",
        "models = pd.DataFrame(models)\n",
        "models['accuracy']=np.nan\n",
        "models['precision score']=np.nan\n",
        "models['recall score']=np.nan\n",
        "models['f1 score']=np.nan\n",
        "models['difference in accuracy (in %)']=np.nan\n",
        "models['difference in recall (in %)']=np.nan\n",
        "models['difference in precision (in %)']=np.nan\n",
        "models['difference in f1 score (in %)']=np.nan\n",
        "models.head()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision score</th>\n",
              "      <th>recall score</th>\n",
              "      <th>f1 score</th>\n",
              "      <th>difference in accuracy (in %)</th>\n",
              "      <th>difference in recall (in %)</th>\n",
              "      <th>difference in precision (in %)</th>\n",
              "      <th>difference in f1 score (in %)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>logistic regression (simple)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>logistic regression (under sampling)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>logistic regression (over sampling)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>logistic regression (smote)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>support vector machine (simple)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   name  ...  difference in f1 score (in %)\n",
              "0          logistic regression (simple)  ...                            NaN\n",
              "1  logistic regression (under sampling)  ...                            NaN\n",
              "2   logistic regression (over sampling)  ...                            NaN\n",
              "3           logistic regression (smote)  ...                            NaN\n",
              "4       support vector machine (simple)  ...                            NaN\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1obDWeyimwBw"
      },
      "source": [
        "a function for implementation of models and computing how well they re performing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDGZvAunm4ll"
      },
      "source": [
        "def get_report(model,x_train,x_test,y_train,y_test,name):\n",
        "  model.fit(x_train,y_train)\n",
        "  y_pred=model.predict(x_test)\n",
        "  print(name,'\\n')\n",
        "  classification_report_m=classification_report(y_test,y_pred)\n",
        "  print(classification_report_m)\n",
        "  confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\n",
        "  print('confusion matrix')\n",
        "  print(confusion_matrix, '\\n')\n",
        "\n",
        "  print('for test data')\n",
        "  accuracy=accuracy_score(y_test,y_pred)\n",
        "  print('accuracy =',accuracy)\n",
        "  precision_score_m=precision_score(y_test,y_pred)\n",
        "  print('presicion score = ',precision_score_m)\n",
        "  recall_score_m=recall_score(y_test,y_pred)\n",
        "  print('recall score =',recall_score_m)\n",
        "  f1_score_m=f1_score(y_test,y_pred)\n",
        "  print('F1 score =',f1_score_m)\n",
        "  print('\\n')\n",
        "\n",
        "  print('for train data')\n",
        "  y_pred_train=model.predict(x_train)\n",
        "  accuracy_t=accuracy_score(y_train,y_pred_train)\n",
        "  print('accuracy =',accuracy_t)\n",
        "  precision_score_m_t=precision_score(y_train,y_pred_train)\n",
        "  print('presicion score = ',precision_score_m_t)\n",
        "  recall_score_m_t=recall_score(y_train,y_pred_train)\n",
        "  print('recall score =',recall_score_m_t)\n",
        "  f1_score_m_t=f1_score(y_train,y_pred_train)\n",
        "  print('F1 score =',f1_score_m_t)\n",
        "  print('\\n')\n",
        "\n",
        "  print('to understand whether our model is overfitting or underfitting')\n",
        "  print('difference in f1 scores')\n",
        "  print(f1_score_m_t,' - ',f1_score_m,' = ',f1_score_m_t-f1_score_m)\n",
        "  print('in percentage = ',(f1_score_m_t-f1_score_m)*100)\n",
        "  print('difference in recall scores')\n",
        "  print(recall_score_m_t,' - ',recall_score_m,' = ',recall_score_m_t-recall_score_m)\n",
        "  print('in percentage = ',(recall_score_m_t-recall_score_m)*100)\n",
        "  print('difference in precision scores')\n",
        "  print(precision_score_m_t,' - ',precision_score_m,' = ',precision_score_m_t-precision_score_m)\n",
        "  print('in percentage = ',(precision_score_m_t-precision_score_m)*100)\n",
        "  print('difference in accuracy scores')\n",
        "  print(accuracy_t,' - ',accuracy,' = ',accuracy_t-accuracy)\n",
        "  print('in percentage = ',(accuracy_t-accuracy)*100)\n",
        "\n",
        "  models.loc[models['name'] == name, 'accuracy'] = accuracy\n",
        "  models.loc[models['name'] == name, 'precision score'] = precision_score_m\n",
        "  models.loc[models['name'] == name, 'recall score'] = recall_score_m\n",
        "  models.loc[models['name'] == name, 'f1 score'] = f1_score_m\n",
        "  models.loc[models['name'] == name, 'difference in f1 score (in %)'] = (f1_score_m_t-f1_score_m)*100\n",
        "  models.loc[models['name'] == name, 'difference in accuracy (in %)'] = (accuracy_t-accuracy)*100\n",
        "  models.loc[models['name'] == name, 'difference in recall (in %)'] = (recall_score_m_t-recall_score_m)*100\n",
        "  models.loc[models['name'] == name, 'difference in precision (in %)'] = (precision_score_m_t-precision_score_m)*100"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHC3hGVAm_Og"
      },
      "source": [
        "due to imbalance in our data, we decided to try 3 methods\n",
        "1. under sampling\n",
        "2. over sampling\n",
        "3. smote\n",
        "to understand how well the balancing techniques work for this data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27B1O-uGnOoQ",
        "outputId": "6808a216-8ee4-4346-df6d-50af70c389c5"
      },
      "source": [
        "rus=RandomUnderSampler()\n",
        "x2_train,y2_train=rus.fit_sample(x_train,y_train)\n",
        "np.bincount(y2_train)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5735, 5735])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e8TlUoxnSRz",
        "outputId": "ff386b85-05aa-4d5c-f5f3-bba13814fe66"
      },
      "source": [
        "ros = RandomOverSampler()\n",
        "x3_train,y3_train=ros.fit_sample(x_train,y_train)\n",
        "np.bincount(y3_train)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([469937, 469937])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5hAWy28nWah",
        "outputId": "5ee4c4df-0c7c-447c-e4ab-50700d62fbdf"
      },
      "source": [
        "smt=SMOTE()\n",
        "x4_train,y4_train=smt.fit_sample(x_train,y_train)\n",
        "np.bincount(y4_train)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([469937, 469937])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFjK09PIniQv"
      },
      "source": [
        "1. model - LOGISTIC REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmPjJYh-nm1E",
        "outputId": "9423482a-da8c-4f81-8025-3444e4e43d62"
      },
      "source": [
        "logistic_regression=LogisticRegression(max_iter=1000)\n",
        "\n",
        "name='logistic regression (simple)'\n",
        "get_report(logistic_regression,x_train,x_test,y_train,y_test,name)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logistic regression (simple) \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00    117454\n",
            "           1       0.88      0.58      0.70      1465\n",
            "\n",
            "    accuracy                           0.99    118919\n",
            "   macro avg       0.94      0.79      0.85    118919\n",
            "weighted avg       0.99      0.99      0.99    118919\n",
            "\n",
            "confusion matrix\n",
            "[[117336    118]\n",
            " [   618    847]] \n",
            "\n",
            "for test data\n",
            "accuracy = 0.9938109133107409\n",
            "presicion score =  0.877720207253886\n",
            "recall score = 0.5781569965870307\n",
            "F1 score = 0.697119341563786\n",
            "\n",
            "\n",
            "for train data\n",
            "accuracy = 0.9939433054709969\n",
            "presicion score =  0.8775132275132275\n",
            "recall score = 0.5783783783783784\n",
            "F1 score = 0.6972149238045192\n",
            "\n",
            "\n",
            "to understand whether our model is overfitting or underfitting\n",
            "difference in f1 scores\n",
            "0.6972149238045192  -  0.697119341563786  =  9.558224073324961e-05\n",
            "in percentage =  0.009558224073324961\n",
            "difference in recall scores\n",
            "0.5783783783783784  -  0.5781569965870307  =  0.00022138179134767455\n",
            "in percentage =  0.022138179134767455\n",
            "difference in precision scores\n",
            "0.8775132275132275  -  0.877720207253886  =  -0.00020697974065853053\n",
            "in percentage =  -0.020697974065853053\n",
            "difference in accuracy scores\n",
            "0.9939433054709969  -  0.9938109133107409  =  0.00013239216025595635\n",
            "in percentage =  0.013239216025595635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COmr4TKnnwZg",
        "outputId": "2754dbcb-a2c7-4ecd-9ee3-9d9e57f4dfa4"
      },
      "source": [
        "name='logistic regression (under sampling)'\n",
        "get_report(logistic_regression,x2_train,x_test,y2_train,y_test,name)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logistic regression (under sampling) \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.97    117454\n",
            "           1       0.18      0.88      0.30      1465\n",
            "\n",
            "    accuracy                           0.95    118919\n",
            "   macro avg       0.59      0.91      0.64    118919\n",
            "weighted avg       0.99      0.95      0.97    118919\n",
            "\n",
            "confusion matrix\n",
            "[[111649   5805]\n",
            " [   182   1283]] \n",
            "\n",
            "for test data\n",
            "accuracy = 0.9496548070535407\n",
            "presicion score =  0.18101015801354403\n",
            "recall score = 0.8757679180887372\n",
            "F1 score = 0.30001169180404536\n",
            "\n",
            "\n",
            "for train data\n",
            "accuracy = 0.9150828247602442\n",
            "presicion score =  0.9516220830961867\n",
            "recall score = 0.8746294681778553\n",
            "F1 score = 0.911502816645466\n",
            "\n",
            "\n",
            "to understand whether our model is overfitting or underfitting\n",
            "difference in f1 scores\n",
            "0.911502816645466  -  0.30001169180404536  =  0.6114911248414205\n",
            "in percentage =  61.14911248414205\n",
            "difference in recall scores\n",
            "0.8746294681778553  -  0.8757679180887372  =  -0.0011384499108819668\n",
            "in percentage =  -0.11384499108819668\n",
            "difference in precision scores\n",
            "0.9516220830961867  -  0.18101015801354403  =  0.7706119250826426\n",
            "in percentage =  77.06119250826427\n",
            "difference in accuracy scores\n",
            "0.9150828247602442  -  0.9496548070535407  =  -0.0345719822932965\n",
            "in percentage =  -3.45719822932965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeRX_HV8n2EY",
        "outputId": "155aea57-0738-4d0d-b8bb-822b25c12a5b"
      },
      "source": [
        "name ='logistic regression (over sampling)'\n",
        "get_report(logistic_regression,x3_train,x_test,y3_train,y_test,name)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logistic regression (over sampling) \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.98    117454\n",
            "           1       0.19      0.87      0.31      1465\n",
            "\n",
            "    accuracy                           0.95    118919\n",
            "   macro avg       0.59      0.91      0.64    118919\n",
            "weighted avg       0.99      0.95      0.97    118919\n",
            "\n",
            "confusion matrix\n",
            "[[111976   5478]\n",
            " [   184   1281]] \n",
            "\n",
            "for test data\n",
            "accuracy = 0.9523877597356184\n",
            "presicion score =  0.18952507767421217\n",
            "recall score = 0.8744027303754266\n",
            "F1 score = 0.31152723735408555\n",
            "\n",
            "\n",
            "for train data\n",
            "accuracy = 0.9131255891747192\n",
            "presicion score =  0.950042653017701\n",
            "recall score = 0.8721105169416326\n",
            "F1 score = 0.9094100352259173\n",
            "\n",
            "\n",
            "to understand whether our model is overfitting or underfitting\n",
            "difference in f1 scores\n",
            "0.9094100352259173  -  0.31152723735408555  =  0.5978827978718317\n",
            "in percentage =  59.78827978718317\n",
            "difference in recall scores\n",
            "0.8721105169416326  -  0.8744027303754266  =  -0.0022922134337940436\n",
            "in percentage =  -0.22922134337940436\n",
            "difference in precision scores\n",
            "0.950042653017701  -  0.18952507767421217  =  0.7605175753434887\n",
            "in percentage =  76.05175753434888\n",
            "difference in accuracy scores\n",
            "0.9131255891747192  -  0.9523877597356184  =  -0.0392621705608992\n",
            "in percentage =  -3.92621705608992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_39wPevyn9Hh",
        "outputId": "7f82dc6f-8218-4997-ce97-184ef4f221d2"
      },
      "source": [
        "name = 'logistic regression (smote)'\n",
        "get_report(logistic_regression,x4_train,x_test,y4_train,y_test,name)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logistic regression (smote) \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.97    117454\n",
            "           1       0.17      0.87      0.29      1465\n",
            "\n",
            "    accuracy                           0.95    118919\n",
            "   macro avg       0.59      0.91      0.63    118919\n",
            "weighted avg       0.99      0.95      0.96    118919\n",
            "\n",
            "confusion matrix\n",
            "[[111369   6085]\n",
            " [   184   1281]] \n",
            "\n",
            "for test data\n",
            "accuracy = 0.9472834450340147\n",
            "presicion score =  0.1739071409177301\n",
            "recall score = 0.8744027303754266\n",
            "F1 score = 0.29011436983354094\n",
            "\n",
            "\n",
            "for train data\n",
            "accuracy = 0.9136958783836983\n",
            "presicion score =  0.9454921676642316\n",
            "recall score = 0.8780091799539087\n",
            "F1 score = 0.9105019898867194\n",
            "\n",
            "\n",
            "to understand whether our model is overfitting or underfitting\n",
            "difference in f1 scores\n",
            "0.9105019898867194  -  0.29011436983354094  =  0.6203876200531784\n",
            "in percentage =  62.038762005317835\n",
            "difference in recall scores\n",
            "0.8780091799539087  -  0.8744027303754266  =  0.003606449578482107\n",
            "in percentage =  0.3606449578482107\n",
            "difference in precision scores\n",
            "0.9454921676642316  -  0.1739071409177301  =  0.7715850267465015\n",
            "in percentage =  77.15850267465015\n",
            "difference in accuracy scores\n",
            "0.9136958783836983  -  0.9472834450340147  =  -0.03358756665031648\n",
            "in percentage =  -3.358756665031648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "wCFcdrwKt_9m",
        "outputId": "2573a050-4c2e-4021-b9c4-e5f2857723c8"
      },
      "source": [
        "models.dropna()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision score</th>\n",
              "      <th>recall score</th>\n",
              "      <th>f1 score</th>\n",
              "      <th>difference in accuracy (in %)</th>\n",
              "      <th>difference in recall (in %)</th>\n",
              "      <th>difference in precision (in %)</th>\n",
              "      <th>difference in f1 score (in %)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>logistic regression (simple)</td>\n",
              "      <td>0.993811</td>\n",
              "      <td>0.877720</td>\n",
              "      <td>0.578157</td>\n",
              "      <td>0.697119</td>\n",
              "      <td>0.013239</td>\n",
              "      <td>0.022138</td>\n",
              "      <td>-0.020698</td>\n",
              "      <td>0.009558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>logistic regression (under sampling)</td>\n",
              "      <td>0.949655</td>\n",
              "      <td>0.181010</td>\n",
              "      <td>0.875768</td>\n",
              "      <td>0.300012</td>\n",
              "      <td>-3.457198</td>\n",
              "      <td>-0.113845</td>\n",
              "      <td>77.061193</td>\n",
              "      <td>61.149112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>logistic regression (over sampling)</td>\n",
              "      <td>0.952388</td>\n",
              "      <td>0.189525</td>\n",
              "      <td>0.874403</td>\n",
              "      <td>0.311527</td>\n",
              "      <td>-3.926217</td>\n",
              "      <td>-0.229221</td>\n",
              "      <td>76.051758</td>\n",
              "      <td>59.788280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>logistic regression (smote)</td>\n",
              "      <td>0.947283</td>\n",
              "      <td>0.173907</td>\n",
              "      <td>0.874403</td>\n",
              "      <td>0.290114</td>\n",
              "      <td>-3.358757</td>\n",
              "      <td>0.360645</td>\n",
              "      <td>77.158503</td>\n",
              "      <td>62.038762</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   name  ...  difference in f1 score (in %)\n",
              "0          logistic regression (simple)  ...                       0.009558\n",
              "1  logistic regression (under sampling)  ...                      61.149112\n",
              "2   logistic regression (over sampling)  ...                      59.788280\n",
              "3           logistic regression (smote)  ...                      62.038762\n",
              "\n",
              "[4 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiIl9mqAoQd8"
      },
      "source": [
        "here we are encountering a problem, which can be written in points as\n",
        "1. if we do not use any sampling technique, we get a decent precision score, a bad recall score (which makes it a bad model)\n",
        "2. but if we use any sampling technique then there is a significant increase in recall but also a significant decrease in precision\n",
        "3. there is no significant difference between over sampling and smote\n",
        "4. also there is a provision for us to say that there is an observable figure denoting over fitting in case of the sampling techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJABJ-G3n_QL"
      },
      "source": [
        "2. model - SUPPORT VECTOR MACHINE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW4DaSvMoK47"
      },
      "source": [
        "#support_vector_machine=SVC()\n",
        "#name='support vector machine (simple)'\n",
        "#get_report(support_vector_machine,x_train,x_test,y_train,y_test,name)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k1RuyIXo7KC"
      },
      "source": [
        "#name='support vector machine (under sampling)'\n",
        "#get_report(support_vector_machine,x2_train,x_test,y2_train,y_test,name)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__YTesxxpG_2"
      },
      "source": [
        "#name='support vector machine (over sampling)'\n",
        "#get_report(support_vector_machine,x3_train,x_test,y3_train,y_test,name)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyId3X6spJ1q"
      },
      "source": [
        "#name='support vector machine (smote)'\n",
        "#get_report(support_vector_machine,x4_train,x_test,y4_train,y_test,name)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1-jKLyWpUmm"
      },
      "source": [
        "3. model - DECISION TREE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHrHpW78pYDi",
        "outputId": "410891ab-faba-4562-b95d-9e7305cf5af2"
      },
      "source": [
        "gini = DecisionTreeClassifier(criterion = \"gini\",random_state = 100)\n",
        "\n",
        "name='decision tree (simple)'\n",
        "get_report(gini,x_train,x_test,y_train,y_test,name)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decision tree (simple) \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    117454\n",
            "           1       0.75      0.77      0.76      1465\n",
            "\n",
            "    accuracy                           0.99    118919\n",
            "   macro avg       0.87      0.88      0.88    118919\n",
            "weighted avg       0.99      0.99      0.99    118919\n",
            "\n",
            "confusion matrix\n",
            "[[117074    380]\n",
            " [   340   1125]] \n",
            "\n",
            "for test data\n",
            "accuracy = 0.9939454586735509\n",
            "presicion score =  0.7475083056478405\n",
            "recall score = 0.7679180887372014\n",
            "F1 score = 0.7575757575757576\n",
            "\n",
            "\n",
            "for train data\n",
            "accuracy = 1.0\n",
            "presicion score =  1.0\n",
            "recall score = 1.0\n",
            "F1 score = 1.0\n",
            "\n",
            "\n",
            "to understand whether our model is overfitting or underfitting\n",
            "difference in f1 scores\n",
            "1.0  -  0.7575757575757576  =  0.24242424242424243\n",
            "in percentage =  24.242424242424242\n",
            "difference in recall scores\n",
            "1.0  -  0.7679180887372014  =  0.23208191126279865\n",
            "in percentage =  23.208191126279864\n",
            "difference in precision scores\n",
            "1.0  -  0.7475083056478405  =  0.25249169435215946\n",
            "in percentage =  25.249169435215947\n",
            "difference in accuracy scores\n",
            "1.0  -  0.9939454586735509  =  0.006054541326449114\n",
            "in percentage =  0.6054541326449114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZpQYfy5pg5b",
        "outputId": "5a183709-56b6-4bd8-d2af-1077c05a6e8f"
      },
      "source": [
        "name='decision tree (under sampling)'\n",
        "get_report(gini,x2_train,x_test,y2_train,y_test,name)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decision tree (under sampling) \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98    117454\n",
            "           1       0.26      0.95      0.41      1465\n",
            "\n",
            "    accuracy                           0.97    118919\n",
            "   macro avg       0.63      0.96      0.70    118919\n",
            "weighted avg       0.99      0.97      0.98    118919\n",
            "\n",
            "confusion matrix\n",
            "[[113515   3939]\n",
            " [    70   1395]] \n",
            "\n",
            "for test data\n",
            "accuracy = 0.9662879775309244\n",
            "presicion score =  0.26152980877390325\n",
            "recall score = 0.9522184300341296\n",
            "F1 score = 0.41035446389174873\n",
            "\n",
            "\n",
            "for train data\n",
            "accuracy = 1.0\n",
            "presicion score =  1.0\n",
            "recall score = 1.0\n",
            "F1 score = 1.0\n",
            "\n",
            "\n",
            "to understand whether our model is overfitting or underfitting\n",
            "difference in f1 scores\n",
            "1.0  -  0.41035446389174873  =  0.5896455361082513\n",
            "in percentage =  58.964553610825135\n",
            "difference in recall scores\n",
            "1.0  -  0.9522184300341296  =  0.04778156996587035\n",
            "in percentage =  4.778156996587035\n",
            "difference in precision scores\n",
            "1.0  -  0.26152980877390325  =  0.7384701912260967\n",
            "in percentage =  73.84701912260967\n",
            "difference in accuracy scores\n",
            "1.0  -  0.9662879775309244  =  0.033712022469075564\n",
            "in percentage =  3.3712022469075564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd2bs_Keplew",
        "outputId": "746608ed-9333-4791-d018-5b9ea0d7e17a"
      },
      "source": [
        "name='decision tree (over sampling)'\n",
        "get_report(gini,x3_train,x_test,y3_train,y_test,name)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decision tree (over sampling) \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    117454\n",
            "           1       0.77      0.74      0.75      1465\n",
            "\n",
            "    accuracy                           0.99    118919\n",
            "   macro avg       0.88      0.87      0.87    118919\n",
            "weighted avg       0.99      0.99      0.99    118919\n",
            "\n",
            "confusion matrix\n",
            "[[117123    331]\n",
            " [   386   1079]] \n",
            "\n",
            "for test data\n",
            "accuracy = 0.9939706859290778\n",
            "presicion score =  0.7652482269503547\n",
            "recall score = 0.736518771331058\n",
            "F1 score = 0.750608695652174\n",
            "\n",
            "\n",
            "for train data\n",
            "accuracy = 1.0\n",
            "presicion score =  1.0\n",
            "recall score = 1.0\n",
            "F1 score = 1.0\n",
            "\n",
            "\n",
            "to understand whether our model is overfitting or underfitting\n",
            "difference in f1 scores\n",
            "1.0  -  0.750608695652174  =  0.24939130434782597\n",
            "in percentage =  24.939130434782598\n",
            "difference in recall scores\n",
            "1.0  -  0.736518771331058  =  0.26348122866894197\n",
            "in percentage =  26.348122866894197\n",
            "difference in precision scores\n",
            "1.0  -  0.7652482269503547  =  0.23475177304964534\n",
            "in percentage =  23.475177304964532\n",
            "difference in accuracy scores\n",
            "1.0  -  0.9939706859290778  =  0.00602931407092222\n",
            "in percentage =  0.602931407092222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g20S50bZpoi6",
        "outputId": "03f25bef-111e-4025-92ef-2172f623df73"
      },
      "source": [
        "name='decision tree (smote)'\n",
        "get_report(gini,x4_train,x_test,y4_train,y_test,name)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decision tree (smote) \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    117454\n",
            "           1       0.73      0.78      0.75      1465\n",
            "\n",
            "    accuracy                           0.99    118919\n",
            "   macro avg       0.86      0.89      0.88    118919\n",
            "weighted avg       0.99      0.99      0.99    118919\n",
            "\n",
            "confusion matrix\n",
            "[[117031    423]\n",
            " [   324   1141]] \n",
            "\n",
            "for test data\n",
            "accuracy = 0.9937184133738091\n",
            "presicion score =  0.729539641943734\n",
            "recall score = 0.778839590443686\n",
            "F1 score = 0.7533839551006932\n",
            "\n",
            "\n",
            "for train data\n",
            "accuracy = 1.0\n",
            "presicion score =  1.0\n",
            "recall score = 1.0\n",
            "F1 score = 1.0\n",
            "\n",
            "\n",
            "to understand whether our model is overfitting or underfitting\n",
            "difference in f1 scores\n",
            "1.0  -  0.7533839551006932  =  0.24661604489930677\n",
            "in percentage =  24.661604489930678\n",
            "difference in recall scores\n",
            "1.0  -  0.778839590443686  =  0.221160409556314\n",
            "in percentage =  22.116040955631398\n",
            "difference in precision scores\n",
            "1.0  -  0.729539641943734  =  0.270460358056266\n",
            "in percentage =  27.0460358056266\n",
            "difference in accuracy scores\n",
            "1.0  -  0.9937184133738091  =  0.006281586626190938\n",
            "in percentage =  0.6281586626190938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3gpUFj_ptY2"
      },
      "source": [
        "4. model - RANDOM FOREST CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yaKVCC1prZe",
        "outputId": "c9b5cd61-27bd-47cb-e779-e938f568ca17"
      },
      "source": [
        "rfc=RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "name='random forest (simple)'\n",
        "get_report(rfc,x_train,x_test,y_train,y_test,name)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random forest (simple) \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    117454\n",
            "           1       0.87      0.76      0.81      1465\n",
            "\n",
            "    accuracy                           1.00    118919\n",
            "   macro avg       0.93      0.88      0.91    118919\n",
            "weighted avg       1.00      1.00      1.00    118919\n",
            "\n",
            "confusion matrix\n",
            "[[117287    167]\n",
            " [   347   1118]] \n",
            "\n",
            "for test data\n",
            "accuracy = 0.9956777302197294\n",
            "presicion score =  0.8700389105058366\n",
            "recall score = 0.7631399317406143\n",
            "F1 score = 0.8130909090909092\n",
            "\n",
            "\n",
            "for train data\n",
            "accuracy = 1.0\n",
            "presicion score =  1.0\n",
            "recall score = 1.0\n",
            "F1 score = 1.0\n",
            "\n",
            "\n",
            "to understand whether our model is overfitting or underfitting\n",
            "difference in f1 scores\n",
            "1.0  -  0.8130909090909092  =  0.1869090909090908\n",
            "in percentage =  18.69090909090908\n",
            "difference in recall scores\n",
            "1.0  -  0.7631399317406143  =  0.23686006825938566\n",
            "in percentage =  23.686006825938566\n",
            "difference in precision scores\n",
            "1.0  -  0.8700389105058366  =  0.1299610894941634\n",
            "in percentage =  12.99610894941634\n",
            "difference in accuracy scores\n",
            "1.0  -  0.9956777302197294  =  0.004322269780270616\n",
            "in percentage =  0.43222697802706156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T00W802UqgAd",
        "outputId": "42589b8d-b4db-4930-d6a9-810e9f8d4099"
      },
      "source": [
        "name='random forest (under sampling)'\n",
        "get_report(rfc,x2_train,x_test,y2_train,y_test,name)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random forest (under sampling) \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98    117454\n",
            "           1       0.26      0.98      0.42      1465\n",
            "\n",
            "    accuracy                           0.97    118919\n",
            "   macro avg       0.63      0.97      0.70    118919\n",
            "weighted avg       0.99      0.97      0.98    118919\n",
            "\n",
            "confusion matrix\n",
            "[[113429   4025]\n",
            " [    27   1438]] \n",
            "\n",
            "for test data\n",
            "accuracy = 0.9659263868683726\n",
            "presicion score =  0.2632253340655318\n",
            "recall score = 0.9815699658703072\n",
            "F1 score = 0.41512702078521946\n",
            "\n",
            "\n",
            "for train data\n",
            "accuracy = 1.0\n",
            "presicion score =  1.0\n",
            "recall score = 1.0\n",
            "F1 score = 1.0\n",
            "\n",
            "\n",
            "to understand whether our model is overfitting or underfitting\n",
            "difference in f1 scores\n",
            "1.0  -  0.41512702078521946  =  0.5848729792147805\n",
            "in percentage =  58.48729792147805\n",
            "difference in recall scores\n",
            "1.0  -  0.9815699658703072  =  0.018430034129692796\n",
            "in percentage =  1.8430034129692796\n",
            "difference in precision scores\n",
            "1.0  -  0.2632253340655318  =  0.7367746659344683\n",
            "in percentage =  73.67746659344682\n",
            "difference in accuracy scores\n",
            "1.0  -  0.9659263868683726  =  0.03407361313162738\n",
            "in percentage =  3.407361313162738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9bJcJSBqj_i",
        "outputId": "65f26dbc-c2a9-441c-d93a-e61b612e49eb"
      },
      "source": [
        "name='random forest (over sampling)'\n",
        "get_report(rfc,x3_train,x_test,y3_train,y_test,name)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random forest (over sampling) \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    117454\n",
            "           1       0.82      0.79      0.80      1465\n",
            "\n",
            "    accuracy                           1.00    118919\n",
            "   macro avg       0.91      0.90      0.90    118919\n",
            "weighted avg       1.00      1.00      1.00    118919\n",
            "\n",
            "confusion matrix\n",
            "[[117193    261]\n",
            " [   303   1162]] \n",
            "\n",
            "for test data\n",
            "accuracy = 0.9952572759609483\n",
            "presicion score =  0.8165846802529867\n",
            "recall score = 0.7931740614334472\n",
            "F1 score = 0.8047091412742382\n",
            "\n",
            "\n",
            "for train data\n",
            "accuracy = 1.0\n",
            "presicion score =  1.0\n",
            "recall score = 1.0\n",
            "F1 score = 1.0\n",
            "\n",
            "\n",
            "to understand whether our model is overfitting or underfitting\n",
            "difference in f1 scores\n",
            "1.0  -  0.8047091412742382  =  0.1952908587257618\n",
            "in percentage =  19.52908587257618\n",
            "difference in recall scores\n",
            "1.0  -  0.7931740614334472  =  0.20682593856655285\n",
            "in percentage =  20.682593856655284\n",
            "difference in precision scores\n",
            "1.0  -  0.8165846802529867  =  0.1834153197470133\n",
            "in percentage =  18.34153197470133\n",
            "difference in accuracy scores\n",
            "1.0  -  0.9952572759609483  =  0.004742724039051738\n",
            "in percentage =  0.4742724039051738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxSyilQTqo3d",
        "outputId": "fcd58b3b-7cd2-4081-9e76-8a769324f3c3"
      },
      "source": [
        "name='random forest (smote)'\n",
        "get_report(rfc,x4_train,x_test,y4_train,y_test,name)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random forest (smote) \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    117454\n",
            "           1       0.81      0.80      0.80      1465\n",
            "\n",
            "    accuracy                           1.00    118919\n",
            "   macro avg       0.90      0.90      0.90    118919\n",
            "weighted avg       1.00      1.00      1.00    118919\n",
            "\n",
            "confusion matrix\n",
            "[[117179    275]\n",
            " [   296   1169]] \n",
            "\n",
            "for test data\n",
            "accuracy = 0.9951984123647188\n",
            "presicion score =  0.8095567867036011\n",
            "recall score = 0.7979522184300342\n",
            "F1 score = 0.8037126160192507\n",
            "\n",
            "\n",
            "for train data\n",
            "accuracy = 1.0\n",
            "presicion score =  1.0\n",
            "recall score = 1.0\n",
            "F1 score = 1.0\n",
            "\n",
            "\n",
            "to understand whether our model is overfitting or underfitting\n",
            "difference in f1 scores\n",
            "1.0  -  0.8037126160192507  =  0.1962873839807493\n",
            "in percentage =  19.62873839807493\n",
            "difference in recall scores\n",
            "1.0  -  0.7979522184300342  =  0.20204778156996583\n",
            "in percentage =  20.204778156996582\n",
            "difference in precision scores\n",
            "1.0  -  0.8095567867036011  =  0.1904432132963989\n",
            "in percentage =  19.044321329639892\n",
            "difference in accuracy scores\n",
            "1.0  -  0.9951984123647188  =  0.004801587635281157\n",
            "in percentage =  0.4801587635281157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5ENqc-8tBy9"
      },
      "source": [
        "5. model - KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ax_OKV1tFu5",
        "outputId": "78ac2e08-3694-4745-9eb9-af83ebda28ea"
      },
      "source": [
        "knn=KNeighborsClassifier()\n",
        "\n",
        "name = 'knn (simple)'\n",
        "get_report(knn,x_train,x_test,y_train,y_test,name)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "knn (simple) \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    117454\n",
            "           1       0.82      0.61      0.70      1465\n",
            "\n",
            "    accuracy                           0.99    118919\n",
            "   macro avg       0.91      0.81      0.85    118919\n",
            "weighted avg       0.99      0.99      0.99    118919\n",
            "\n",
            "confusion matrix\n",
            "[[117261    193]\n",
            " [   568    897]] \n",
            "\n",
            "for test data\n",
            "accuracy = 0.9936006861813503\n",
            "presicion score =  0.8229357798165138\n",
            "recall score = 0.6122866894197952\n",
            "F1 score = 0.7021526418786693\n",
            "\n",
            "\n",
            "for train data\n",
            "accuracy = 0.9947379707025009\n",
            "presicion score =  0.8747680890538033\n",
            "recall score = 0.6577157802964254\n",
            "F1 score = 0.7508709067383299\n",
            "\n",
            "\n",
            "to understand whether our model is overfitting or underfitting\n",
            "difference in f1 scores\n",
            "0.7508709067383299  -  0.7021526418786693  =  0.048718264859660576\n",
            "in percentage =  4.871826485966057\n",
            "difference in recall scores\n",
            "0.6577157802964254  -  0.6122866894197952  =  0.04542909087663016\n",
            "in percentage =  4.542909087663016\n",
            "difference in precision scores\n",
            "0.8747680890538033  -  0.8229357798165138  =  0.05183230923728954\n",
            "in percentage =  5.183230923728955\n",
            "difference in accuracy scores\n",
            "0.9947379707025009  -  0.9936006861813503  =  0.0011372845211505833\n",
            "in percentage =  0.11372845211505833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jU9XMZKtL6g",
        "outputId": "ae0d2639-ff17-45f1-d6ed-a5eaece67538"
      },
      "source": [
        "name = 'knn (under sampling)'\n",
        "get_report(knn,x2_train,x_test,y2_train,y_test,name)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "knn (under sampling) \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.98    117454\n",
            "           1       0.20      0.88      0.32      1465\n",
            "\n",
            "    accuracy                           0.95    118919\n",
            "   macro avg       0.60      0.92      0.65    118919\n",
            "weighted avg       0.99      0.95      0.97    118919\n",
            "\n",
            "confusion matrix\n",
            "[[112119   5335]\n",
            " [   172   1293]] \n",
            "\n",
            "for test data\n",
            "accuracy = 0.95369116793784\n",
            "presicion score =  0.19508147254073627\n",
            "recall score = 0.8825938566552901\n",
            "F1 score = 0.31953540096379585\n",
            "\n",
            "\n",
            "for train data\n",
            "accuracy = 0.9369659982563209\n",
            "presicion score =  0.9630450849963045\n",
            "recall score = 0.9088055797733217\n",
            "F1 score = 0.9351394994168833\n",
            "\n",
            "\n",
            "to understand whether our model is overfitting or underfitting\n",
            "difference in f1 scores\n",
            "0.9351394994168833  -  0.31953540096379585  =  0.6156040984530875\n",
            "in percentage =  61.56040984530875\n",
            "difference in recall scores\n",
            "0.9088055797733217  -  0.8825938566552901  =  0.02621172311803155\n",
            "in percentage =  2.621172311803155\n",
            "difference in precision scores\n",
            "0.9630450849963045  -  0.19508147254073627  =  0.7679636124555682\n",
            "in percentage =  76.79636124555682\n",
            "difference in accuracy scores\n",
            "0.9369659982563209  -  0.95369116793784  =  -0.01672516968151916\n",
            "in percentage =  -1.6725169681519159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7QSSnjCtSo7",
        "outputId": "370e8857-dadf-4bd3-fc74-5b3fdc804f87"
      },
      "source": [
        "name = 'knn (over sampling)'\n",
        "get_report(knn,x3_train,x_test,y3_train,y_test,name)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "knn (over sampling) \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99    117454\n",
            "           1       0.50      0.75      0.60      1465\n",
            "\n",
            "    accuracy                           0.99    118919\n",
            "   macro avg       0.75      0.87      0.80    118919\n",
            "weighted avg       0.99      0.99      0.99    118919\n",
            "\n",
            "confusion matrix\n",
            "[[116349   1105]\n",
            " [   362   1103]] \n",
            "\n",
            "for test data\n",
            "accuracy = 0.98766387204736\n",
            "presicion score =  0.4995471014492754\n",
            "recall score = 0.752901023890785\n",
            "F1 score = 0.6005989654233597\n",
            "\n",
            "\n",
            "for train data\n",
            "accuracy = 0.9966410391180094\n",
            "presicion score =  0.9933269075490283\n",
            "recall score = 1.0\n",
            "F1 score = 0.9966522839652143\n",
            "\n",
            "\n",
            "to understand whether our model is overfitting or underfitting\n",
            "difference in f1 scores\n",
            "0.9966522839652143  -  0.6005989654233597  =  0.39605331854185466\n",
            "in percentage =  39.605331854185465\n",
            "difference in recall scores\n",
            "1.0  -  0.752901023890785  =  0.24709897610921505\n",
            "in percentage =  24.709897610921505\n",
            "difference in precision scores\n",
            "0.9933269075490283  -  0.4995471014492754  =  0.4937798060997529\n",
            "in percentage =  49.37798060997529\n",
            "difference in accuracy scores\n",
            "0.9966410391180094  -  0.98766387204736  =  0.008977167070649439\n",
            "in percentage =  0.8977167070649439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ElYMvfptZ5L",
        "outputId": "ad96059d-5391-49b4-8434-42ed9e885b25"
      },
      "source": [
        "name = 'knn (smote)'\n",
        "get_report(knn,x4_train,x_test,y4_train,y_test,name)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "knn (smote) \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99    117454\n",
            "           1       0.32      0.84      0.46      1465\n",
            "\n",
            "    accuracy                           0.98    118919\n",
            "   macro avg       0.66      0.91      0.73    118919\n",
            "weighted avg       0.99      0.98      0.98    118919\n",
            "\n",
            "confusion matrix\n",
            "[[114833   2621]\n",
            " [   235   1230]] \n",
            "\n",
            "for test data\n",
            "accuracy = 0.9759836527384186\n",
            "presicion score =  0.3193975590755648\n",
            "recall score = 0.8395904436860068\n",
            "F1 score = 0.4627539503386004\n",
            "\n",
            "\n",
            "for train data\n",
            "accuracy = 0.9909062278560743\n",
            "presicion score =  0.9829003114743118\n",
            "recall score = 0.9991956368619623\n",
            "F1 score = 0.9909809901178159\n",
            "\n",
            "\n",
            "to understand whether our model is overfitting or underfitting\n",
            "difference in f1 scores\n",
            "0.9909809901178159  -  0.4627539503386004  =  0.5282270397792155\n",
            "in percentage =  52.82270397792155\n",
            "difference in recall scores\n",
            "0.9991956368619623  -  0.8395904436860068  =  0.15960519317595556\n",
            "in percentage =  15.960519317595557\n",
            "difference in precision scores\n",
            "0.9829003114743118  -  0.3193975590755648  =  0.663502752398747\n",
            "in percentage =  66.3502752398747\n",
            "difference in accuracy scores\n",
            "0.9909062278560743  -  0.9759836527384186  =  0.014922575117655712\n",
            "in percentage =  1.4922575117655712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "snMtKEG9smVT",
        "outputId": "032247d0-8814-40ad-8100-07462420724a"
      },
      "source": [
        "models_used = models.dropna()\n",
        "models_used"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision score</th>\n",
              "      <th>recall score</th>\n",
              "      <th>f1 score</th>\n",
              "      <th>difference in accuracy (in %)</th>\n",
              "      <th>difference in recall (in %)</th>\n",
              "      <th>difference in precision (in %)</th>\n",
              "      <th>difference in f1 score (in %)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>logistic regression (simple)</td>\n",
              "      <td>0.993811</td>\n",
              "      <td>0.877720</td>\n",
              "      <td>0.578157</td>\n",
              "      <td>0.697119</td>\n",
              "      <td>0.013239</td>\n",
              "      <td>0.022138</td>\n",
              "      <td>-0.020698</td>\n",
              "      <td>0.009558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>logistic regression (under sampling)</td>\n",
              "      <td>0.949655</td>\n",
              "      <td>0.181010</td>\n",
              "      <td>0.875768</td>\n",
              "      <td>0.300012</td>\n",
              "      <td>-3.457198</td>\n",
              "      <td>-0.113845</td>\n",
              "      <td>77.061193</td>\n",
              "      <td>61.149112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>logistic regression (over sampling)</td>\n",
              "      <td>0.952388</td>\n",
              "      <td>0.189525</td>\n",
              "      <td>0.874403</td>\n",
              "      <td>0.311527</td>\n",
              "      <td>-3.926217</td>\n",
              "      <td>-0.229221</td>\n",
              "      <td>76.051758</td>\n",
              "      <td>59.788280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>logistic regression (smote)</td>\n",
              "      <td>0.947283</td>\n",
              "      <td>0.173907</td>\n",
              "      <td>0.874403</td>\n",
              "      <td>0.290114</td>\n",
              "      <td>-3.358757</td>\n",
              "      <td>0.360645</td>\n",
              "      <td>77.158503</td>\n",
              "      <td>62.038762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>decision tree (simple)</td>\n",
              "      <td>0.993945</td>\n",
              "      <td>0.747508</td>\n",
              "      <td>0.767918</td>\n",
              "      <td>0.757576</td>\n",
              "      <td>0.605454</td>\n",
              "      <td>23.208191</td>\n",
              "      <td>25.249169</td>\n",
              "      <td>24.242424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>decision tree (under sampling)</td>\n",
              "      <td>0.966288</td>\n",
              "      <td>0.261530</td>\n",
              "      <td>0.952218</td>\n",
              "      <td>0.410354</td>\n",
              "      <td>3.371202</td>\n",
              "      <td>4.778157</td>\n",
              "      <td>73.847019</td>\n",
              "      <td>58.964554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>decision tree (over sampling)</td>\n",
              "      <td>0.993971</td>\n",
              "      <td>0.765248</td>\n",
              "      <td>0.736519</td>\n",
              "      <td>0.750609</td>\n",
              "      <td>0.602931</td>\n",
              "      <td>26.348123</td>\n",
              "      <td>23.475177</td>\n",
              "      <td>24.939130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>decision tree (smote)</td>\n",
              "      <td>0.993718</td>\n",
              "      <td>0.729540</td>\n",
              "      <td>0.778840</td>\n",
              "      <td>0.753384</td>\n",
              "      <td>0.628159</td>\n",
              "      <td>22.116041</td>\n",
              "      <td>27.046036</td>\n",
              "      <td>24.661604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>random forest (simple)</td>\n",
              "      <td>0.995678</td>\n",
              "      <td>0.870039</td>\n",
              "      <td>0.763140</td>\n",
              "      <td>0.813091</td>\n",
              "      <td>0.432227</td>\n",
              "      <td>23.686007</td>\n",
              "      <td>12.996109</td>\n",
              "      <td>18.690909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>random forest (under sampling)</td>\n",
              "      <td>0.965926</td>\n",
              "      <td>0.263225</td>\n",
              "      <td>0.981570</td>\n",
              "      <td>0.415127</td>\n",
              "      <td>3.407361</td>\n",
              "      <td>1.843003</td>\n",
              "      <td>73.677467</td>\n",
              "      <td>58.487298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>random forest (over sampling)</td>\n",
              "      <td>0.995257</td>\n",
              "      <td>0.816585</td>\n",
              "      <td>0.793174</td>\n",
              "      <td>0.804709</td>\n",
              "      <td>0.474272</td>\n",
              "      <td>20.682594</td>\n",
              "      <td>18.341532</td>\n",
              "      <td>19.529086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>random forest (smote)</td>\n",
              "      <td>0.995198</td>\n",
              "      <td>0.809557</td>\n",
              "      <td>0.797952</td>\n",
              "      <td>0.803713</td>\n",
              "      <td>0.480159</td>\n",
              "      <td>20.204778</td>\n",
              "      <td>19.044321</td>\n",
              "      <td>19.628738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>knn (simple)</td>\n",
              "      <td>0.993601</td>\n",
              "      <td>0.822936</td>\n",
              "      <td>0.612287</td>\n",
              "      <td>0.702153</td>\n",
              "      <td>0.113728</td>\n",
              "      <td>4.542909</td>\n",
              "      <td>5.183231</td>\n",
              "      <td>4.871826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>knn (under sampling)</td>\n",
              "      <td>0.953691</td>\n",
              "      <td>0.195081</td>\n",
              "      <td>0.882594</td>\n",
              "      <td>0.319535</td>\n",
              "      <td>-1.672517</td>\n",
              "      <td>2.621172</td>\n",
              "      <td>76.796361</td>\n",
              "      <td>61.560410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>knn (over sampling)</td>\n",
              "      <td>0.987664</td>\n",
              "      <td>0.499547</td>\n",
              "      <td>0.752901</td>\n",
              "      <td>0.600599</td>\n",
              "      <td>0.897717</td>\n",
              "      <td>24.709898</td>\n",
              "      <td>49.377981</td>\n",
              "      <td>39.605332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>knn (smote)</td>\n",
              "      <td>0.975984</td>\n",
              "      <td>0.319398</td>\n",
              "      <td>0.839590</td>\n",
              "      <td>0.462754</td>\n",
              "      <td>1.492258</td>\n",
              "      <td>15.960519</td>\n",
              "      <td>66.350275</td>\n",
              "      <td>52.822704</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    name  ...  difference in f1 score (in %)\n",
              "0           logistic regression (simple)  ...                       0.009558\n",
              "1   logistic regression (under sampling)  ...                      61.149112\n",
              "2    logistic regression (over sampling)  ...                      59.788280\n",
              "3            logistic regression (smote)  ...                      62.038762\n",
              "8                 decision tree (simple)  ...                      24.242424\n",
              "9         decision tree (under sampling)  ...                      58.964554\n",
              "10         decision tree (over sampling)  ...                      24.939130\n",
              "11                 decision tree (smote)  ...                      24.661604\n",
              "12                random forest (simple)  ...                      18.690909\n",
              "13        random forest (under sampling)  ...                      58.487298\n",
              "14         random forest (over sampling)  ...                      19.529086\n",
              "15                 random forest (smote)  ...                      19.628738\n",
              "20                          knn (simple)  ...                       4.871826\n",
              "21                  knn (under sampling)  ...                      61.560410\n",
              "22                   knn (over sampling)  ...                      39.605332\n",
              "23                           knn (smote)  ...                      52.822704\n",
              "\n",
              "[16 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "RhKhSWGOPvl8",
        "outputId": "9eb53a36-1b6d-471a-853c-8fdc5ccda9e1"
      },
      "source": [
        "models_used.sort_values(by='f1 score',ascending=False)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision score</th>\n",
              "      <th>recall score</th>\n",
              "      <th>f1 score</th>\n",
              "      <th>difference in accuracy (in %)</th>\n",
              "      <th>difference in recall (in %)</th>\n",
              "      <th>difference in precision (in %)</th>\n",
              "      <th>difference in f1 score (in %)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>random forest (simple)</td>\n",
              "      <td>0.995678</td>\n",
              "      <td>0.870039</td>\n",
              "      <td>0.763140</td>\n",
              "      <td>0.813091</td>\n",
              "      <td>0.432227</td>\n",
              "      <td>23.686007</td>\n",
              "      <td>12.996109</td>\n",
              "      <td>18.690909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>random forest (over sampling)</td>\n",
              "      <td>0.995257</td>\n",
              "      <td>0.816585</td>\n",
              "      <td>0.793174</td>\n",
              "      <td>0.804709</td>\n",
              "      <td>0.474272</td>\n",
              "      <td>20.682594</td>\n",
              "      <td>18.341532</td>\n",
              "      <td>19.529086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>random forest (smote)</td>\n",
              "      <td>0.995198</td>\n",
              "      <td>0.809557</td>\n",
              "      <td>0.797952</td>\n",
              "      <td>0.803713</td>\n",
              "      <td>0.480159</td>\n",
              "      <td>20.204778</td>\n",
              "      <td>19.044321</td>\n",
              "      <td>19.628738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>decision tree (simple)</td>\n",
              "      <td>0.993945</td>\n",
              "      <td>0.747508</td>\n",
              "      <td>0.767918</td>\n",
              "      <td>0.757576</td>\n",
              "      <td>0.605454</td>\n",
              "      <td>23.208191</td>\n",
              "      <td>25.249169</td>\n",
              "      <td>24.242424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>decision tree (smote)</td>\n",
              "      <td>0.993718</td>\n",
              "      <td>0.729540</td>\n",
              "      <td>0.778840</td>\n",
              "      <td>0.753384</td>\n",
              "      <td>0.628159</td>\n",
              "      <td>22.116041</td>\n",
              "      <td>27.046036</td>\n",
              "      <td>24.661604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>decision tree (over sampling)</td>\n",
              "      <td>0.993971</td>\n",
              "      <td>0.765248</td>\n",
              "      <td>0.736519</td>\n",
              "      <td>0.750609</td>\n",
              "      <td>0.602931</td>\n",
              "      <td>26.348123</td>\n",
              "      <td>23.475177</td>\n",
              "      <td>24.939130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>knn (simple)</td>\n",
              "      <td>0.993601</td>\n",
              "      <td>0.822936</td>\n",
              "      <td>0.612287</td>\n",
              "      <td>0.702153</td>\n",
              "      <td>0.113728</td>\n",
              "      <td>4.542909</td>\n",
              "      <td>5.183231</td>\n",
              "      <td>4.871826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>logistic regression (simple)</td>\n",
              "      <td>0.993811</td>\n",
              "      <td>0.877720</td>\n",
              "      <td>0.578157</td>\n",
              "      <td>0.697119</td>\n",
              "      <td>0.013239</td>\n",
              "      <td>0.022138</td>\n",
              "      <td>-0.020698</td>\n",
              "      <td>0.009558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>knn (over sampling)</td>\n",
              "      <td>0.987664</td>\n",
              "      <td>0.499547</td>\n",
              "      <td>0.752901</td>\n",
              "      <td>0.600599</td>\n",
              "      <td>0.897717</td>\n",
              "      <td>24.709898</td>\n",
              "      <td>49.377981</td>\n",
              "      <td>39.605332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>knn (smote)</td>\n",
              "      <td>0.975984</td>\n",
              "      <td>0.319398</td>\n",
              "      <td>0.839590</td>\n",
              "      <td>0.462754</td>\n",
              "      <td>1.492258</td>\n",
              "      <td>15.960519</td>\n",
              "      <td>66.350275</td>\n",
              "      <td>52.822704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>random forest (under sampling)</td>\n",
              "      <td>0.965926</td>\n",
              "      <td>0.263225</td>\n",
              "      <td>0.981570</td>\n",
              "      <td>0.415127</td>\n",
              "      <td>3.407361</td>\n",
              "      <td>1.843003</td>\n",
              "      <td>73.677467</td>\n",
              "      <td>58.487298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>decision tree (under sampling)</td>\n",
              "      <td>0.966288</td>\n",
              "      <td>0.261530</td>\n",
              "      <td>0.952218</td>\n",
              "      <td>0.410354</td>\n",
              "      <td>3.371202</td>\n",
              "      <td>4.778157</td>\n",
              "      <td>73.847019</td>\n",
              "      <td>58.964554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>knn (under sampling)</td>\n",
              "      <td>0.953691</td>\n",
              "      <td>0.195081</td>\n",
              "      <td>0.882594</td>\n",
              "      <td>0.319535</td>\n",
              "      <td>-1.672517</td>\n",
              "      <td>2.621172</td>\n",
              "      <td>76.796361</td>\n",
              "      <td>61.560410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>logistic regression (over sampling)</td>\n",
              "      <td>0.952388</td>\n",
              "      <td>0.189525</td>\n",
              "      <td>0.874403</td>\n",
              "      <td>0.311527</td>\n",
              "      <td>-3.926217</td>\n",
              "      <td>-0.229221</td>\n",
              "      <td>76.051758</td>\n",
              "      <td>59.788280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>logistic regression (under sampling)</td>\n",
              "      <td>0.949655</td>\n",
              "      <td>0.181010</td>\n",
              "      <td>0.875768</td>\n",
              "      <td>0.300012</td>\n",
              "      <td>-3.457198</td>\n",
              "      <td>-0.113845</td>\n",
              "      <td>77.061193</td>\n",
              "      <td>61.149112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>logistic regression (smote)</td>\n",
              "      <td>0.947283</td>\n",
              "      <td>0.173907</td>\n",
              "      <td>0.874403</td>\n",
              "      <td>0.290114</td>\n",
              "      <td>-3.358757</td>\n",
              "      <td>0.360645</td>\n",
              "      <td>77.158503</td>\n",
              "      <td>62.038762</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    name  ...  difference in f1 score (in %)\n",
              "12                random forest (simple)  ...                      18.690909\n",
              "14         random forest (over sampling)  ...                      19.529086\n",
              "15                 random forest (smote)  ...                      19.628738\n",
              "8                 decision tree (simple)  ...                      24.242424\n",
              "11                 decision tree (smote)  ...                      24.661604\n",
              "10         decision tree (over sampling)  ...                      24.939130\n",
              "20                          knn (simple)  ...                       4.871826\n",
              "0           logistic regression (simple)  ...                       0.009558\n",
              "22                   knn (over sampling)  ...                      39.605332\n",
              "23                           knn (smote)  ...                      52.822704\n",
              "13        random forest (under sampling)  ...                      58.487298\n",
              "9         decision tree (under sampling)  ...                      58.964554\n",
              "21                  knn (under sampling)  ...                      61.560410\n",
              "2    logistic regression (over sampling)  ...                      59.788280\n",
              "1   logistic regression (under sampling)  ...                      61.149112\n",
              "3            logistic regression (smote)  ...                      62.038762\n",
              "\n",
              "[16 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "Rv-PZmYvRW7y",
        "outputId": "e5015780-4417-470a-fa06-c0271f6e14cb"
      },
      "source": [
        "models_used.sort_values(by='recall score',ascending=False)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision score</th>\n",
              "      <th>recall score</th>\n",
              "      <th>f1 score</th>\n",
              "      <th>difference in accuracy (in %)</th>\n",
              "      <th>difference in recall (in %)</th>\n",
              "      <th>difference in precision (in %)</th>\n",
              "      <th>difference in f1 score (in %)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>random forest (under sampling)</td>\n",
              "      <td>0.965926</td>\n",
              "      <td>0.263225</td>\n",
              "      <td>0.981570</td>\n",
              "      <td>0.415127</td>\n",
              "      <td>3.407361</td>\n",
              "      <td>1.843003</td>\n",
              "      <td>73.677467</td>\n",
              "      <td>58.487298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>decision tree (under sampling)</td>\n",
              "      <td>0.966288</td>\n",
              "      <td>0.261530</td>\n",
              "      <td>0.952218</td>\n",
              "      <td>0.410354</td>\n",
              "      <td>3.371202</td>\n",
              "      <td>4.778157</td>\n",
              "      <td>73.847019</td>\n",
              "      <td>58.964554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>knn (under sampling)</td>\n",
              "      <td>0.953691</td>\n",
              "      <td>0.195081</td>\n",
              "      <td>0.882594</td>\n",
              "      <td>0.319535</td>\n",
              "      <td>-1.672517</td>\n",
              "      <td>2.621172</td>\n",
              "      <td>76.796361</td>\n",
              "      <td>61.560410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>logistic regression (under sampling)</td>\n",
              "      <td>0.949655</td>\n",
              "      <td>0.181010</td>\n",
              "      <td>0.875768</td>\n",
              "      <td>0.300012</td>\n",
              "      <td>-3.457198</td>\n",
              "      <td>-0.113845</td>\n",
              "      <td>77.061193</td>\n",
              "      <td>61.149112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>logistic regression (over sampling)</td>\n",
              "      <td>0.952388</td>\n",
              "      <td>0.189525</td>\n",
              "      <td>0.874403</td>\n",
              "      <td>0.311527</td>\n",
              "      <td>-3.926217</td>\n",
              "      <td>-0.229221</td>\n",
              "      <td>76.051758</td>\n",
              "      <td>59.788280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>logistic regression (smote)</td>\n",
              "      <td>0.947283</td>\n",
              "      <td>0.173907</td>\n",
              "      <td>0.874403</td>\n",
              "      <td>0.290114</td>\n",
              "      <td>-3.358757</td>\n",
              "      <td>0.360645</td>\n",
              "      <td>77.158503</td>\n",
              "      <td>62.038762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>knn (smote)</td>\n",
              "      <td>0.975984</td>\n",
              "      <td>0.319398</td>\n",
              "      <td>0.839590</td>\n",
              "      <td>0.462754</td>\n",
              "      <td>1.492258</td>\n",
              "      <td>15.960519</td>\n",
              "      <td>66.350275</td>\n",
              "      <td>52.822704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>random forest (smote)</td>\n",
              "      <td>0.995198</td>\n",
              "      <td>0.809557</td>\n",
              "      <td>0.797952</td>\n",
              "      <td>0.803713</td>\n",
              "      <td>0.480159</td>\n",
              "      <td>20.204778</td>\n",
              "      <td>19.044321</td>\n",
              "      <td>19.628738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>random forest (over sampling)</td>\n",
              "      <td>0.995257</td>\n",
              "      <td>0.816585</td>\n",
              "      <td>0.793174</td>\n",
              "      <td>0.804709</td>\n",
              "      <td>0.474272</td>\n",
              "      <td>20.682594</td>\n",
              "      <td>18.341532</td>\n",
              "      <td>19.529086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>decision tree (smote)</td>\n",
              "      <td>0.993718</td>\n",
              "      <td>0.729540</td>\n",
              "      <td>0.778840</td>\n",
              "      <td>0.753384</td>\n",
              "      <td>0.628159</td>\n",
              "      <td>22.116041</td>\n",
              "      <td>27.046036</td>\n",
              "      <td>24.661604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>decision tree (simple)</td>\n",
              "      <td>0.993945</td>\n",
              "      <td>0.747508</td>\n",
              "      <td>0.767918</td>\n",
              "      <td>0.757576</td>\n",
              "      <td>0.605454</td>\n",
              "      <td>23.208191</td>\n",
              "      <td>25.249169</td>\n",
              "      <td>24.242424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>random forest (simple)</td>\n",
              "      <td>0.995678</td>\n",
              "      <td>0.870039</td>\n",
              "      <td>0.763140</td>\n",
              "      <td>0.813091</td>\n",
              "      <td>0.432227</td>\n",
              "      <td>23.686007</td>\n",
              "      <td>12.996109</td>\n",
              "      <td>18.690909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>knn (over sampling)</td>\n",
              "      <td>0.987664</td>\n",
              "      <td>0.499547</td>\n",
              "      <td>0.752901</td>\n",
              "      <td>0.600599</td>\n",
              "      <td>0.897717</td>\n",
              "      <td>24.709898</td>\n",
              "      <td>49.377981</td>\n",
              "      <td>39.605332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>decision tree (over sampling)</td>\n",
              "      <td>0.993971</td>\n",
              "      <td>0.765248</td>\n",
              "      <td>0.736519</td>\n",
              "      <td>0.750609</td>\n",
              "      <td>0.602931</td>\n",
              "      <td>26.348123</td>\n",
              "      <td>23.475177</td>\n",
              "      <td>24.939130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>knn (simple)</td>\n",
              "      <td>0.993601</td>\n",
              "      <td>0.822936</td>\n",
              "      <td>0.612287</td>\n",
              "      <td>0.702153</td>\n",
              "      <td>0.113728</td>\n",
              "      <td>4.542909</td>\n",
              "      <td>5.183231</td>\n",
              "      <td>4.871826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>logistic regression (simple)</td>\n",
              "      <td>0.993811</td>\n",
              "      <td>0.877720</td>\n",
              "      <td>0.578157</td>\n",
              "      <td>0.697119</td>\n",
              "      <td>0.013239</td>\n",
              "      <td>0.022138</td>\n",
              "      <td>-0.020698</td>\n",
              "      <td>0.009558</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    name  ...  difference in f1 score (in %)\n",
              "13        random forest (under sampling)  ...                      58.487298\n",
              "9         decision tree (under sampling)  ...                      58.964554\n",
              "21                  knn (under sampling)  ...                      61.560410\n",
              "1   logistic regression (under sampling)  ...                      61.149112\n",
              "2    logistic regression (over sampling)  ...                      59.788280\n",
              "3            logistic regression (smote)  ...                      62.038762\n",
              "23                           knn (smote)  ...                      52.822704\n",
              "15                 random forest (smote)  ...                      19.628738\n",
              "14         random forest (over sampling)  ...                      19.529086\n",
              "11                 decision tree (smote)  ...                      24.661604\n",
              "8                 decision tree (simple)  ...                      24.242424\n",
              "12                random forest (simple)  ...                      18.690909\n",
              "22                   knn (over sampling)  ...                      39.605332\n",
              "10         decision tree (over sampling)  ...                      24.939130\n",
              "20                          knn (simple)  ...                       4.871826\n",
              "0           logistic regression (simple)  ...                       0.009558\n",
              "\n",
              "[16 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "Zz7smHD9V3Xi",
        "outputId": "34fefb0c-ff20-436e-c532-c4654c8f2e91"
      },
      "source": [
        "models_used.sort_values(by='difference in f1 score (in %)',ascending=False)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision score</th>\n",
              "      <th>recall score</th>\n",
              "      <th>f1 score</th>\n",
              "      <th>difference in accuracy (in %)</th>\n",
              "      <th>difference in recall (in %)</th>\n",
              "      <th>difference in precision (in %)</th>\n",
              "      <th>difference in f1 score (in %)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>logistic regression (smote)</td>\n",
              "      <td>0.947283</td>\n",
              "      <td>0.173907</td>\n",
              "      <td>0.874403</td>\n",
              "      <td>0.290114</td>\n",
              "      <td>-3.358757</td>\n",
              "      <td>0.360645</td>\n",
              "      <td>77.158503</td>\n",
              "      <td>62.038762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>knn (under sampling)</td>\n",
              "      <td>0.953691</td>\n",
              "      <td>0.195081</td>\n",
              "      <td>0.882594</td>\n",
              "      <td>0.319535</td>\n",
              "      <td>-1.672517</td>\n",
              "      <td>2.621172</td>\n",
              "      <td>76.796361</td>\n",
              "      <td>61.560410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>logistic regression (under sampling)</td>\n",
              "      <td>0.949655</td>\n",
              "      <td>0.181010</td>\n",
              "      <td>0.875768</td>\n",
              "      <td>0.300012</td>\n",
              "      <td>-3.457198</td>\n",
              "      <td>-0.113845</td>\n",
              "      <td>77.061193</td>\n",
              "      <td>61.149112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>logistic regression (over sampling)</td>\n",
              "      <td>0.952388</td>\n",
              "      <td>0.189525</td>\n",
              "      <td>0.874403</td>\n",
              "      <td>0.311527</td>\n",
              "      <td>-3.926217</td>\n",
              "      <td>-0.229221</td>\n",
              "      <td>76.051758</td>\n",
              "      <td>59.788280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>decision tree (under sampling)</td>\n",
              "      <td>0.966288</td>\n",
              "      <td>0.261530</td>\n",
              "      <td>0.952218</td>\n",
              "      <td>0.410354</td>\n",
              "      <td>3.371202</td>\n",
              "      <td>4.778157</td>\n",
              "      <td>73.847019</td>\n",
              "      <td>58.964554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>random forest (under sampling)</td>\n",
              "      <td>0.965926</td>\n",
              "      <td>0.263225</td>\n",
              "      <td>0.981570</td>\n",
              "      <td>0.415127</td>\n",
              "      <td>3.407361</td>\n",
              "      <td>1.843003</td>\n",
              "      <td>73.677467</td>\n",
              "      <td>58.487298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>knn (smote)</td>\n",
              "      <td>0.975984</td>\n",
              "      <td>0.319398</td>\n",
              "      <td>0.839590</td>\n",
              "      <td>0.462754</td>\n",
              "      <td>1.492258</td>\n",
              "      <td>15.960519</td>\n",
              "      <td>66.350275</td>\n",
              "      <td>52.822704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>knn (over sampling)</td>\n",
              "      <td>0.987664</td>\n",
              "      <td>0.499547</td>\n",
              "      <td>0.752901</td>\n",
              "      <td>0.600599</td>\n",
              "      <td>0.897717</td>\n",
              "      <td>24.709898</td>\n",
              "      <td>49.377981</td>\n",
              "      <td>39.605332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>decision tree (over sampling)</td>\n",
              "      <td>0.993971</td>\n",
              "      <td>0.765248</td>\n",
              "      <td>0.736519</td>\n",
              "      <td>0.750609</td>\n",
              "      <td>0.602931</td>\n",
              "      <td>26.348123</td>\n",
              "      <td>23.475177</td>\n",
              "      <td>24.939130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>decision tree (smote)</td>\n",
              "      <td>0.993718</td>\n",
              "      <td>0.729540</td>\n",
              "      <td>0.778840</td>\n",
              "      <td>0.753384</td>\n",
              "      <td>0.628159</td>\n",
              "      <td>22.116041</td>\n",
              "      <td>27.046036</td>\n",
              "      <td>24.661604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>decision tree (simple)</td>\n",
              "      <td>0.993945</td>\n",
              "      <td>0.747508</td>\n",
              "      <td>0.767918</td>\n",
              "      <td>0.757576</td>\n",
              "      <td>0.605454</td>\n",
              "      <td>23.208191</td>\n",
              "      <td>25.249169</td>\n",
              "      <td>24.242424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>random forest (smote)</td>\n",
              "      <td>0.995198</td>\n",
              "      <td>0.809557</td>\n",
              "      <td>0.797952</td>\n",
              "      <td>0.803713</td>\n",
              "      <td>0.480159</td>\n",
              "      <td>20.204778</td>\n",
              "      <td>19.044321</td>\n",
              "      <td>19.628738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>random forest (over sampling)</td>\n",
              "      <td>0.995257</td>\n",
              "      <td>0.816585</td>\n",
              "      <td>0.793174</td>\n",
              "      <td>0.804709</td>\n",
              "      <td>0.474272</td>\n",
              "      <td>20.682594</td>\n",
              "      <td>18.341532</td>\n",
              "      <td>19.529086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>random forest (simple)</td>\n",
              "      <td>0.995678</td>\n",
              "      <td>0.870039</td>\n",
              "      <td>0.763140</td>\n",
              "      <td>0.813091</td>\n",
              "      <td>0.432227</td>\n",
              "      <td>23.686007</td>\n",
              "      <td>12.996109</td>\n",
              "      <td>18.690909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>knn (simple)</td>\n",
              "      <td>0.993601</td>\n",
              "      <td>0.822936</td>\n",
              "      <td>0.612287</td>\n",
              "      <td>0.702153</td>\n",
              "      <td>0.113728</td>\n",
              "      <td>4.542909</td>\n",
              "      <td>5.183231</td>\n",
              "      <td>4.871826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>logistic regression (simple)</td>\n",
              "      <td>0.993811</td>\n",
              "      <td>0.877720</td>\n",
              "      <td>0.578157</td>\n",
              "      <td>0.697119</td>\n",
              "      <td>0.013239</td>\n",
              "      <td>0.022138</td>\n",
              "      <td>-0.020698</td>\n",
              "      <td>0.009558</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    name  ...  difference in f1 score (in %)\n",
              "3            logistic regression (smote)  ...                      62.038762\n",
              "21                  knn (under sampling)  ...                      61.560410\n",
              "1   logistic regression (under sampling)  ...                      61.149112\n",
              "2    logistic regression (over sampling)  ...                      59.788280\n",
              "9         decision tree (under sampling)  ...                      58.964554\n",
              "13        random forest (under sampling)  ...                      58.487298\n",
              "23                           knn (smote)  ...                      52.822704\n",
              "22                   knn (over sampling)  ...                      39.605332\n",
              "10         decision tree (over sampling)  ...                      24.939130\n",
              "11                 decision tree (smote)  ...                      24.661604\n",
              "8                 decision tree (simple)  ...                      24.242424\n",
              "15                 random forest (smote)  ...                      19.628738\n",
              "14         random forest (over sampling)  ...                      19.529086\n",
              "12                random forest (simple)  ...                      18.690909\n",
              "20                          knn (simple)  ...                       4.871826\n",
              "0           logistic regression (simple)  ...                       0.009558\n",
              "\n",
              "[16 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "prfe8Mj3WBsu",
        "outputId": "81c6e8aa-8211-43a7-9c2d-37214431cea2"
      },
      "source": [
        "models_used.sort_values(by='difference in recall (in %)',ascending=False)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision score</th>\n",
              "      <th>recall score</th>\n",
              "      <th>f1 score</th>\n",
              "      <th>difference in accuracy (in %)</th>\n",
              "      <th>difference in recall (in %)</th>\n",
              "      <th>difference in precision (in %)</th>\n",
              "      <th>difference in f1 score (in %)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>decision tree (over sampling)</td>\n",
              "      <td>0.993971</td>\n",
              "      <td>0.765248</td>\n",
              "      <td>0.736519</td>\n",
              "      <td>0.750609</td>\n",
              "      <td>0.602931</td>\n",
              "      <td>26.348123</td>\n",
              "      <td>23.475177</td>\n",
              "      <td>24.939130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>knn (over sampling)</td>\n",
              "      <td>0.987664</td>\n",
              "      <td>0.499547</td>\n",
              "      <td>0.752901</td>\n",
              "      <td>0.600599</td>\n",
              "      <td>0.897717</td>\n",
              "      <td>24.709898</td>\n",
              "      <td>49.377981</td>\n",
              "      <td>39.605332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>random forest (simple)</td>\n",
              "      <td>0.995678</td>\n",
              "      <td>0.870039</td>\n",
              "      <td>0.763140</td>\n",
              "      <td>0.813091</td>\n",
              "      <td>0.432227</td>\n",
              "      <td>23.686007</td>\n",
              "      <td>12.996109</td>\n",
              "      <td>18.690909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>decision tree (simple)</td>\n",
              "      <td>0.993945</td>\n",
              "      <td>0.747508</td>\n",
              "      <td>0.767918</td>\n",
              "      <td>0.757576</td>\n",
              "      <td>0.605454</td>\n",
              "      <td>23.208191</td>\n",
              "      <td>25.249169</td>\n",
              "      <td>24.242424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>decision tree (smote)</td>\n",
              "      <td>0.993718</td>\n",
              "      <td>0.729540</td>\n",
              "      <td>0.778840</td>\n",
              "      <td>0.753384</td>\n",
              "      <td>0.628159</td>\n",
              "      <td>22.116041</td>\n",
              "      <td>27.046036</td>\n",
              "      <td>24.661604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>random forest (over sampling)</td>\n",
              "      <td>0.995257</td>\n",
              "      <td>0.816585</td>\n",
              "      <td>0.793174</td>\n",
              "      <td>0.804709</td>\n",
              "      <td>0.474272</td>\n",
              "      <td>20.682594</td>\n",
              "      <td>18.341532</td>\n",
              "      <td>19.529086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>random forest (smote)</td>\n",
              "      <td>0.995198</td>\n",
              "      <td>0.809557</td>\n",
              "      <td>0.797952</td>\n",
              "      <td>0.803713</td>\n",
              "      <td>0.480159</td>\n",
              "      <td>20.204778</td>\n",
              "      <td>19.044321</td>\n",
              "      <td>19.628738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>knn (smote)</td>\n",
              "      <td>0.975984</td>\n",
              "      <td>0.319398</td>\n",
              "      <td>0.839590</td>\n",
              "      <td>0.462754</td>\n",
              "      <td>1.492258</td>\n",
              "      <td>15.960519</td>\n",
              "      <td>66.350275</td>\n",
              "      <td>52.822704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>decision tree (under sampling)</td>\n",
              "      <td>0.966288</td>\n",
              "      <td>0.261530</td>\n",
              "      <td>0.952218</td>\n",
              "      <td>0.410354</td>\n",
              "      <td>3.371202</td>\n",
              "      <td>4.778157</td>\n",
              "      <td>73.847019</td>\n",
              "      <td>58.964554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>knn (simple)</td>\n",
              "      <td>0.993601</td>\n",
              "      <td>0.822936</td>\n",
              "      <td>0.612287</td>\n",
              "      <td>0.702153</td>\n",
              "      <td>0.113728</td>\n",
              "      <td>4.542909</td>\n",
              "      <td>5.183231</td>\n",
              "      <td>4.871826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>knn (under sampling)</td>\n",
              "      <td>0.953691</td>\n",
              "      <td>0.195081</td>\n",
              "      <td>0.882594</td>\n",
              "      <td>0.319535</td>\n",
              "      <td>-1.672517</td>\n",
              "      <td>2.621172</td>\n",
              "      <td>76.796361</td>\n",
              "      <td>61.560410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>random forest (under sampling)</td>\n",
              "      <td>0.965926</td>\n",
              "      <td>0.263225</td>\n",
              "      <td>0.981570</td>\n",
              "      <td>0.415127</td>\n",
              "      <td>3.407361</td>\n",
              "      <td>1.843003</td>\n",
              "      <td>73.677467</td>\n",
              "      <td>58.487298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>logistic regression (smote)</td>\n",
              "      <td>0.947283</td>\n",
              "      <td>0.173907</td>\n",
              "      <td>0.874403</td>\n",
              "      <td>0.290114</td>\n",
              "      <td>-3.358757</td>\n",
              "      <td>0.360645</td>\n",
              "      <td>77.158503</td>\n",
              "      <td>62.038762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>logistic regression (simple)</td>\n",
              "      <td>0.993811</td>\n",
              "      <td>0.877720</td>\n",
              "      <td>0.578157</td>\n",
              "      <td>0.697119</td>\n",
              "      <td>0.013239</td>\n",
              "      <td>0.022138</td>\n",
              "      <td>-0.020698</td>\n",
              "      <td>0.009558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>logistic regression (under sampling)</td>\n",
              "      <td>0.949655</td>\n",
              "      <td>0.181010</td>\n",
              "      <td>0.875768</td>\n",
              "      <td>0.300012</td>\n",
              "      <td>-3.457198</td>\n",
              "      <td>-0.113845</td>\n",
              "      <td>77.061193</td>\n",
              "      <td>61.149112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>logistic regression (over sampling)</td>\n",
              "      <td>0.952388</td>\n",
              "      <td>0.189525</td>\n",
              "      <td>0.874403</td>\n",
              "      <td>0.311527</td>\n",
              "      <td>-3.926217</td>\n",
              "      <td>-0.229221</td>\n",
              "      <td>76.051758</td>\n",
              "      <td>59.788280</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    name  ...  difference in f1 score (in %)\n",
              "10         decision tree (over sampling)  ...                      24.939130\n",
              "22                   knn (over sampling)  ...                      39.605332\n",
              "12                random forest (simple)  ...                      18.690909\n",
              "8                 decision tree (simple)  ...                      24.242424\n",
              "11                 decision tree (smote)  ...                      24.661604\n",
              "14         random forest (over sampling)  ...                      19.529086\n",
              "15                 random forest (smote)  ...                      19.628738\n",
              "23                           knn (smote)  ...                      52.822704\n",
              "9         decision tree (under sampling)  ...                      58.964554\n",
              "20                          knn (simple)  ...                       4.871826\n",
              "21                  knn (under sampling)  ...                      61.560410\n",
              "13        random forest (under sampling)  ...                      58.487298\n",
              "3            logistic regression (smote)  ...                      62.038762\n",
              "0           logistic regression (simple)  ...                       0.009558\n",
              "1   logistic regression (under sampling)  ...                      61.149112\n",
              "2    logistic regression (over sampling)  ...                      59.788280\n",
              "\n",
              "[16 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    }
  ]
}