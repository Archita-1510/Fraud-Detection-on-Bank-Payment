{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IId2t79TkKX4",
    "outputId": "83a8be39-414d-4525-e406-876e17f3166f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import (SMOTE, RandomOverSampler)\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, recall_score, precision_score, accuracy_score, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import io\n",
    "df = pd.read_csv(r\"E:\\sem 2\\Project\\Banksim Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "sElO1Q2dkWGs",
    "outputId": "657c12d0-c7f5-4b8a-ae7a-b4d105c5082f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>customer</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>zipcodeOri</th>\n",
       "      <th>merchant</th>\n",
       "      <th>zipMerchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amount</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>'C1093826151'</td>\n",
       "      <td>'4'</td>\n",
       "      <td>'M'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'M348934600'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'es_transportation'</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>'C352968107'</td>\n",
       "      <td>'2'</td>\n",
       "      <td>'M'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'M348934600'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'es_transportation'</td>\n",
       "      <td>39.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>'C2054744914'</td>\n",
       "      <td>'4'</td>\n",
       "      <td>'F'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'M1823072687'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'es_transportation'</td>\n",
       "      <td>26.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>'C1760612790'</td>\n",
       "      <td>'3'</td>\n",
       "      <td>'M'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'M348934600'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'es_transportation'</td>\n",
       "      <td>17.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>'C757503768'</td>\n",
       "      <td>'5'</td>\n",
       "      <td>'M'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'M348934600'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'es_transportation'</td>\n",
       "      <td>35.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step       customer  age gender zipcodeOri       merchant zipMerchant  \\\n",
       "0     0  'C1093826151'  '4'    'M'    '28007'   'M348934600'     '28007'   \n",
       "1     0   'C352968107'  '2'    'M'    '28007'   'M348934600'     '28007'   \n",
       "2     0  'C2054744914'  '4'    'F'    '28007'  'M1823072687'     '28007'   \n",
       "3     0  'C1760612790'  '3'    'M'    '28007'   'M348934600'     '28007'   \n",
       "4     0   'C757503768'  '5'    'M'    '28007'   'M348934600'     '28007'   \n",
       "\n",
       "              category  amount  fraud  \n",
       "0  'es_transportation'    4.55      0  \n",
       "1  'es_transportation'   39.68      0  \n",
       "2  'es_transportation'   26.89      0  \n",
       "3  'es_transportation'   17.25      0  \n",
       "4  'es_transportation'   35.72      0  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2dfJwMLklBG"
   },
   "source": [
    "modifying the data frame according to our needs\n",
    "1. changing the categorical variables to numerical type for the ease of computation\n",
    "2. dropping the columns with merchant and customer zip code because both the columns take only 1 value for the entire data set\n",
    "3. dropping transactions with amount=0, because if there has been no transaction at all there is no question of fraud in that case, having such data might be misleading and increases the non fraud cases unnecessarilly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "_sPoqQdalEbp",
    "outputId": "e87a0c70-01ea-43d2-a6ab-171e6daf92a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>customer</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amount</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>210</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2753</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>39.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2285</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>26.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1650</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>17.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3585</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>35.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step  customer  age  gender  merchant  category  amount  fraud\n",
       "0     0       210    4       2        30        12    4.55      0\n",
       "1     0      2753    2       2        30        12   39.68      0\n",
       "2     0      2285    4       1        18        12   26.89      0\n",
       "3     0      1650    3       2        30        12   17.25      0\n",
       "4     0      3585    5       2        30        12   35.72      0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder=LabelEncoder()\n",
    "df['customer']=encoder.fit_transform(df['customer'])\n",
    "df['age']=encoder.fit_transform(df['age'])\n",
    "df['gender']=encoder.fit_transform(df['gender'])\n",
    "df['merchant']=encoder.fit_transform(df['merchant'])\n",
    "df['category']=encoder.fit_transform(df['category'])\n",
    "df.drop(['zipcodeOri','zipMerchant'],axis=1,inplace=True)\n",
    "df=df.query('amount>0') #give filtered dataframe according to condition.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uImjUSsgrQrJ"
   },
   "source": [
    "splitting the data frame into 2 other data frame, one for fraud and one for non fraud cases respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UL2RvpVwrJwc",
    "outputId": "ec151c68-141b-46a6-83d0-16ec87b04f93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200, 8) (587391, 8)\n"
     ]
    }
   ],
   "source": [
    "fraud = df[df['fraud']==1]\n",
    "non_fraud = df[df['fraud']==0]\n",
    "print(fraud.shape , non_fraud.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVIjeTV1lXBq"
   },
   "source": [
    "splitting the data from into dependent (y) and independent variables (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93wHoO1xld0N",
    "outputId": "727fb588-9f75-4f7a-ec22-875e467e2d09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   step  customer  age  gender  merchant  category  amount\n",
      "0     0       210    4       2        30        12    4.55\n",
      "1     0      2753    2       2        30        12   39.68\n",
      "2     0      2285    4       1        18        12   26.89\n",
      "3     0      1650    3       2        30        12   17.25\n",
      "4     0      3585    5       2        30        12   35.72\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: fraud, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x=df.drop('fraud',axis=1)\n",
    "y=df['fraud']\n",
    "print(x.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23dTx2Mflllf"
   },
   "source": [
    "correlation between the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "bK4V0QEpltTi",
    "outputId": "4e576376-d731-4ec1-f6e6-2002184a509e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ce16399f08>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqf0R-fzl3wt"
   },
   "source": [
    "checking whether fraud is dependent on other categorical variables or not, using chi square test of independence for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lyrG_G99mDih",
    "outputId": "57c0a898-3643-4831-c772-551dae09f333"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi sq test of independence between fraud and steps\n",
      "p value is 0.999999936271118\n",
      "Independent (H0 holds true)\n",
      "\n",
      "\n",
      "chi sq test of independence between fraud and customer\n",
      "p value is 0.0\n",
      "Dependent (reject H0)\n",
      "\n",
      "\n",
      "chi sq test of independence between fraud age\n",
      "p value is 2.00866504430546e-07\n",
      "Dependent (reject H0)\n",
      "\n",
      "\n",
      "chi sq test of independence between fraud and gender\n",
      "p value is 5.932305999855071e-85\n",
      "Dependent (reject H0)\n",
      "\n",
      "\n",
      "chi sq test of independence between fraud and merchant\n",
      "p value is 0.0\n",
      "Dependent (reject H0)\n",
      "\n",
      "\n",
      "chi sq test of independence between fraud and category\n",
      "p value is 0.0\n",
      "Dependent (reject H0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "chisq=df\n",
    "alpha=0.05\n",
    "\n",
    "print('chi sq test of independence between fraud and steps')\n",
    "chisq_table=pd.crosstab(chisq['fraud'],chisq['step'])\n",
    "stat, p, dof, expected = chi2_contingency(chisq_table)\n",
    "# interpret p-value\n",
    "print(\"p value is \" + str(p))\n",
    "if p <= alpha:\n",
    "     print('Dependent (reject H0)')\n",
    "else:\n",
    "     print('Independent (H0 holds true)')\n",
    "print('\\n')\n",
    "\n",
    "print('chi sq test of independence between fraud and customer')\n",
    "chisq_table=pd.crosstab(chisq['fraud'],chisq['customer'])\n",
    "stat, p, dof, expected = chi2_contingency(chisq_table)\n",
    "# interpret p-value\n",
    "print(\"p value is \" + str(p))\n",
    "if p <= alpha:\n",
    "     print('Dependent (reject H0)')\n",
    "else:\n",
    "     print('Independent (H0 holds true)')\n",
    "print('\\n')\n",
    "\n",
    "print('chi sq test of independence between fraud age')\n",
    "chisq_table=pd.crosstab(chisq['fraud'],chisq['age'])\n",
    "stat, p, dof, expected = chi2_contingency(chisq_table)\n",
    "# interpret p-value\n",
    "print(\"p value is \" + str(p))\n",
    "if p <= alpha:\n",
    "     print('Dependent (reject H0)')\n",
    "else:\n",
    "     print('Independent (H0 holds true)')\n",
    "print('\\n')\n",
    "\n",
    "print('chi sq test of independence between fraud and gender')\n",
    "chisq_table=pd.crosstab(chisq['fraud'],chisq['gender'])\n",
    "stat, p, dof, expected = chi2_contingency(chisq_table)\n",
    "# interpret p-value\n",
    "print(\"p value is \" + str(p))\n",
    "if p <= alpha:\n",
    "     print('Dependent (reject H0)')\n",
    "else:\n",
    "     print('Independent (H0 holds true)')\n",
    "print('\\n')\n",
    "\n",
    "print('chi sq test of independence between fraud and merchant')\n",
    "chisq_table=pd.crosstab(chisq['fraud'],chisq['merchant'])\n",
    "stat, p, dof, expected = chi2_contingency(chisq_table)\n",
    "# interpret p-value\n",
    "print(\"p value is \" + str(p))\n",
    "if p <= alpha:\n",
    "     print('Dependent (reject H0)')\n",
    "else:\n",
    "     print('Independent (H0 holds true)')\n",
    "print('\\n')\n",
    "\n",
    "print('chi sq test of independence between fraud and category')\n",
    "chisq_table=pd.crosstab(chisq['fraud'],chisq['category'])\n",
    "stat, p, dof, expected = chi2_contingency(chisq_table)\n",
    "# interpret p-value\n",
    "print(\"p value is \" + str(p))\n",
    "if p <= alpha:\n",
    "     print('Dependent (reject H0)')\n",
    "else:\n",
    "     print('Independent (H0 holds true)')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovbEDy98mbQk"
   },
   "source": [
    "splitting into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "0jYb4sR_l2ap"
   },
   "outputs": [],
   "source": [
    "x_new=pd.DataFrame(data=df,columns=['amount','category','merchant','gender','age','customer'])\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_new,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRkNbLNXmiek"
   },
   "source": [
    "creating a new data frame for the ease of keeping track of how each models work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "nq2ClW3Glh3G",
    "outputId": "4a549e16-c81c-450e-97a2-9a4ab3b11a8f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision score</th>\n",
       "      <th>recall score</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>difference in accuracy (in %)</th>\n",
       "      <th>difference in recall (in %)</th>\n",
       "      <th>difference in precision (in %)</th>\n",
       "      <th>difference in f1 score (in %)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic regression (simple)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic regression (under sampling)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic regression (over sampling)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic regression (smote)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>support vector machine (simple)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   name  accuracy  precision score  \\\n",
       "0          logistic regression (simple)       NaN              NaN   \n",
       "1  logistic regression (under sampling)       NaN              NaN   \n",
       "2   logistic regression (over sampling)       NaN              NaN   \n",
       "3           logistic regression (smote)       NaN              NaN   \n",
       "4       support vector machine (simple)       NaN              NaN   \n",
       "\n",
       "   recall score  f1 score  difference in accuracy (in %)  \\\n",
       "0           NaN       NaN                            NaN   \n",
       "1           NaN       NaN                            NaN   \n",
       "2           NaN       NaN                            NaN   \n",
       "3           NaN       NaN                            NaN   \n",
       "4           NaN       NaN                            NaN   \n",
       "\n",
       "   difference in recall (in %)  difference in precision (in %)  \\\n",
       "0                          NaN                             NaN   \n",
       "1                          NaN                             NaN   \n",
       "2                          NaN                             NaN   \n",
       "3                          NaN                             NaN   \n",
       "4                          NaN                             NaN   \n",
       "\n",
       "   difference in f1 score (in %)  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4                            NaN  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models={'name':['logistic regression (simple)','logistic regression (under sampling)','logistic regression (over sampling)','logistic regression (smote)','support vector machine (simple)','support vector machine (under sampling)','support vector machine (over sampling)','support vector machine (smote)','decision tree (simple)','decision tree (under sampling)','decision tree (over sampling)','decision tree (smote)','random forest (simple)','random forest (under sampling)','random forest (over sampling)','random forest (smote)','naive bayes (simple)','naive bayes (under sampling)','naive bayes (over sampling)','naive bayes (smote)','knn (simple)','knn (under sampling)','knn (over sampling)','knn (smote)','Gradient boosting']}\n",
    "models = pd.DataFrame(models)\n",
    "models['accuracy']=np.nan\n",
    "models['precision score']=np.nan\n",
    "models['recall score']=np.nan\n",
    "models['f1 score']=np.nan\n",
    "models['difference in accuracy (in %)']=np.nan\n",
    "models['difference in recall (in %)']=np.nan\n",
    "models['difference in precision (in %)']=np.nan\n",
    "models['difference in f1 score (in %)']=np.nan\n",
    "models.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1obDWeyimwBw"
   },
   "source": [
    "a function for implementation of models and computing how well they re performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "wDGZvAunm4ll"
   },
   "outputs": [],
   "source": [
    "def get_report(model,x_train,x_test,y_train,y_test,name):\n",
    "  model.fit(x_train,y_train)\n",
    "  y_pred=model.predict(x_test)\n",
    "  print(name,'\\n')\n",
    "  classification_report_m=classification_report(y_test,y_pred)\n",
    "  print(classification_report_m)\n",
    "  confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\n",
    "  print('confusion matrix')\n",
    "  print(confusion_matrix, '\\n')\n",
    "\n",
    "  print('for test data')\n",
    "  accuracy=accuracy_score(y_test,y_pred)\n",
    "  print('accuracy =',accuracy)\n",
    "  precision_score_m=precision_score(y_test,y_pred)\n",
    "  print('presicion score = ',precision_score_m)\n",
    "  recall_score_m=recall_score(y_test,y_pred)\n",
    "  print('recall score =',recall_score_m)\n",
    "  f1_score_m=f1_score(y_test,y_pred)\n",
    "  print('F1 score =',f1_score_m)\n",
    "  print('\\n')\n",
    "\n",
    "  print('for train data')\n",
    "  y_pred_train=model.predict(x_train)\n",
    "  accuracy_t=accuracy_score(y_train,y_pred_train)\n",
    "  print('accuracy =',accuracy_t)\n",
    "  precision_score_m_t=precision_score(y_train,y_pred_train)\n",
    "  print('presicion score = ',precision_score_m_t)\n",
    "  recall_score_m_t=recall_score(y_train,y_pred_train)\n",
    "  print('recall score =',recall_score_m_t)\n",
    "  f1_score_m_t=f1_score(y_train,y_pred_train)\n",
    "  print('F1 score =',f1_score_m_t)\n",
    "  print('\\n')\n",
    "\n",
    "  print('to understand whether our model is overfitting or underfitting')\n",
    "  print('difference in f1 scores')\n",
    "  print(f1_score_m_t,' - ',f1_score_m,' = ',f1_score_m_t-f1_score_m)\n",
    "  print('in percentage = ',(f1_score_m_t-f1_score_m)*100)\n",
    "  print('difference in recall scores')\n",
    "  print(recall_score_m_t,' - ',recall_score_m,' = ',recall_score_m_t-recall_score_m)\n",
    "  print('in percentage = ',(recall_score_m_t-recall_score_m)*100)\n",
    "  print('difference in precision scores')\n",
    "  print(precision_score_m_t,' - ',precision_score_m,' = ',precision_score_m_t-precision_score_m)\n",
    "  print('in percentage = ',(precision_score_m_t-precision_score_m)*100)\n",
    "  print('difference in accuracy scores')\n",
    "  print(accuracy_t,' - ',accuracy,' = ',accuracy_t-accuracy)\n",
    "  print('in percentage = ',(accuracy_t-accuracy)*100)\n",
    "\n",
    "  models.loc[models['name'] == name, 'accuracy'] = accuracy\n",
    "  models.loc[models['name'] == name, 'precision score'] = precision_score_m\n",
    "  models.loc[models['name'] == name, 'recall score'] = recall_score_m\n",
    "  models.loc[models['name'] == name, 'f1 score'] = f1_score_m\n",
    "  models.loc[models['name'] == name, 'difference in f1 score (in %)'] = (f1_score_m_t-f1_score_m)*100\n",
    "  models.loc[models['name'] == name, 'difference in accuracy (in %)'] = (accuracy_t-accuracy)*100\n",
    "  models.loc[models['name'] == name, 'difference in recall (in %)'] = (recall_score_m_t-recall_score_m)*100\n",
    "  models.loc[models['name'] == name, 'difference in precision (in %)'] = (precision_score_m_t-precision_score_m)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHC3hGVAm_Og"
   },
   "source": [
    "due to imbalance in our data, we decided to try 3 methods\n",
    "1. under sampling\n",
    "2. over sampling\n",
    "3. smote\n",
    "to understand how well the balancing techniques work for this data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27B1O-uGnOoQ",
    "outputId": "6808a216-8ee4-4346-df6d-50af70c389c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5735, 5735], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus=RandomUnderSampler()\n",
    "x2_train,y2_train=rus.fit_resample(x_train,y_train)\n",
    "np.bincount(y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e8TlUoxnSRz",
    "outputId": "ff386b85-05aa-4d5c-f5f3-bba13814fe66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([469937, 469937], dtype=int64)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ros = RandomOverSampler()\n",
    "x3_train,y3_train=ros.fit_resample(x_train,y_train)\n",
    "np.bincount(y3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5hAWy28nWah",
    "outputId": "5ee4c4df-0c7c-447c-e4ab-50700d62fbdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([469937, 469937], dtype=int64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smt=SMOTE()\n",
    "x4_train,y4_train=smt.fit_resample(x_train,y_train)\n",
    "np.bincount(y4_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFjK09PIniQv"
   },
   "source": [
    "1. model - LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qmPjJYh-nm1E",
    "outputId": "9423482a-da8c-4f81-8025-3444e4e43d62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression (simple) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    117454\n",
      "           1       0.88      0.58      0.70      1465\n",
      "\n",
      "    accuracy                           0.99    118919\n",
      "   macro avg       0.94      0.79      0.85    118919\n",
      "weighted avg       0.99      0.99      0.99    118919\n",
      "\n",
      "confusion matrix\n",
      "[[117336    118]\n",
      " [   618    847]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.9938109133107409\n",
      "presicion score =  0.877720207253886\n",
      "recall score = 0.5781569965870307\n",
      "F1 score = 0.697119341563786\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 0.9939454077599691\n",
      "presicion score =  0.877745435300344\n",
      "recall score = 0.5783783783783784\n",
      "F1 score = 0.6972882068530585\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "0.6972882068530585  -  0.697119341563786  =  0.00016886528927251288\n",
      "in percentage =  0.016886528927251288\n",
      "difference in recall scores\n",
      "0.5783783783783784  -  0.5781569965870307  =  0.00022138179134767455\n",
      "in percentage =  0.022138179134767455\n",
      "difference in precision scores\n",
      "0.877745435300344  -  0.877720207253886  =  2.522804645799237e-05\n",
      "in percentage =  0.002522804645799237\n",
      "difference in accuracy scores\n",
      "0.9939454077599691  -  0.9938109133107409  =  0.0001344944492281952\n",
      "in percentage =  0.01344944492281952\n"
     ]
    }
   ],
   "source": [
    "logistic_regression=LogisticRegression(max_iter=1000)\n",
    "\n",
    "name='logistic regression (simple)'\n",
    "get_report(logistic_regression,x_train,x_test,y_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COmr4TKnnwZg",
    "outputId": "2754dbcb-a2c7-4ecd-9ee3-9d9e57f4dfa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression (under sampling) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97    117454\n",
      "           1       0.16      0.88      0.27      1465\n",
      "\n",
      "    accuracy                           0.94    118919\n",
      "   macro avg       0.58      0.91      0.62    118919\n",
      "weighted avg       0.99      0.94      0.96    118919\n",
      "\n",
      "confusion matrix\n",
      "[[110754   6700]\n",
      " [   170   1295]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.9422295848434649\n",
      "presicion score =  0.16197623514696685\n",
      "recall score = 0.8839590443686007\n",
      "F1 score = 0.273784355179704\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 0.9142981691368788\n",
      "presicion score =  0.947289156626506\n",
      "recall score = 0.8774193548387097\n",
      "F1 score = 0.9110165655834163\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "0.9110165655834163  -  0.273784355179704  =  0.6372322104037123\n",
      "in percentage =  63.72322104037123\n",
      "difference in recall scores\n",
      "0.8774193548387097  -  0.8839590443686007  =  -0.006539689529891035\n",
      "in percentage =  -0.6539689529891035\n",
      "difference in precision scores\n",
      "0.947289156626506  -  0.16197623514696685  =  0.7853129214795391\n",
      "in percentage =  78.5312921479539\n",
      "difference in accuracy scores\n",
      "0.9142981691368788  -  0.9422295848434649  =  -0.027931415706586082\n",
      "in percentage =  -2.7931415706586082\n"
     ]
    }
   ],
   "source": [
    "name='logistic regression (under sampling)'\n",
    "get_report(logistic_regression,x2_train,x_test,y2_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EeRX_HV8n2EY",
    "outputId": "155aea57-0738-4d0d-b8bb-822b25c12a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression (over sampling) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98    117454\n",
      "           1       0.19      0.88      0.31      1465\n",
      "\n",
      "    accuracy                           0.95    118919\n",
      "   macro avg       0.59      0.91      0.64    118919\n",
      "weighted avg       0.99      0.95      0.97    118919\n",
      "\n",
      "confusion matrix\n",
      "[[111950   5504]\n",
      " [   183   1282]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.9521775326062277\n",
      "presicion score =  0.18891836133215442\n",
      "recall score = 0.875085324232082\n",
      "F1 score = 0.3107502120955036\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 0.9132979527042987\n",
      "presicion score =  0.9499258710155671\n",
      "recall score = 0.8725935604134171\n",
      "F1 score = 0.9096190511048483\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "0.9096190511048483  -  0.3107502120955036  =  0.5988688390093447\n",
      "in percentage =  59.88688390093447\n",
      "difference in recall scores\n",
      "0.8725935604134171  -  0.875085324232082  =  -0.002491763818664894\n",
      "in percentage =  -0.24917638186648938\n",
      "difference in precision scores\n",
      "0.9499258710155671  -  0.18891836133215442  =  0.7610075096834127\n",
      "in percentage =  76.10075096834127\n",
      "difference in accuracy scores\n",
      "0.9132979527042987  -  0.9521775326062277  =  -0.03887957990192903\n",
      "in percentage =  -3.887957990192903\n"
     ]
    }
   ],
   "source": [
    "name ='logistic regression (over sampling)'\n",
    "get_report(logistic_regression,x3_train,x_test,y3_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_39wPevyn9Hh",
    "outputId": "7f82dc6f-8218-4997-ce97-184ef4f221d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression (smote) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97    117454\n",
      "           1       0.18      0.87      0.30      1465\n",
      "\n",
      "    accuracy                           0.95    118919\n",
      "   macro avg       0.59      0.91      0.64    118919\n",
      "weighted avg       0.99      0.95      0.97    118919\n",
      "\n",
      "confusion matrix\n",
      "[[111679   5775]\n",
      " [   191   1274]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.9498313978422287\n",
      "presicion score =  0.1807348560079444\n",
      "recall score = 0.8696245733788396\n",
      "F1 score = 0.2992717876438807\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 0.9209542981293237\n",
      "presicion score =  0.9489974715492548\n",
      "recall score = 0.8897256440756952\n",
      "F1 score = 0.9184062346585424\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "0.9184062346585424  -  0.2992717876438807  =  0.6191344470146618\n",
      "in percentage =  61.91344470146618\n",
      "difference in recall scores\n",
      "0.8897256440756952  -  0.8696245733788396  =  0.020101070696855627\n",
      "in percentage =  2.0101070696855627\n",
      "difference in precision scores\n",
      "0.9489974715492548  -  0.1807348560079444  =  0.7682626155413105\n",
      "in percentage =  76.82626155413105\n",
      "difference in accuracy scores\n",
      "0.9209542981293237  -  0.9498313978422287  =  -0.028877099712904974\n",
      "in percentage =  -2.8877099712904974\n"
     ]
    }
   ],
   "source": [
    "name = 'logistic regression (smote)'\n",
    "get_report(logistic_regression,x4_train,x_test,y4_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "wCFcdrwKt_9m",
    "outputId": "2573a050-4c2e-4021-b9c4-e5f2857723c8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision score</th>\n",
       "      <th>recall score</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>difference in accuracy (in %)</th>\n",
       "      <th>difference in recall (in %)</th>\n",
       "      <th>difference in precision (in %)</th>\n",
       "      <th>difference in f1 score (in %)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic regression (simple)</td>\n",
       "      <td>0.993811</td>\n",
       "      <td>0.877720</td>\n",
       "      <td>0.578157</td>\n",
       "      <td>0.697119</td>\n",
       "      <td>0.013449</td>\n",
       "      <td>0.022138</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.016887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic regression (under sampling)</td>\n",
       "      <td>0.942230</td>\n",
       "      <td>0.161976</td>\n",
       "      <td>0.883959</td>\n",
       "      <td>0.273784</td>\n",
       "      <td>-2.793142</td>\n",
       "      <td>-0.653969</td>\n",
       "      <td>78.531292</td>\n",
       "      <td>63.723221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic regression (over sampling)</td>\n",
       "      <td>0.952178</td>\n",
       "      <td>0.188918</td>\n",
       "      <td>0.875085</td>\n",
       "      <td>0.310750</td>\n",
       "      <td>-3.887958</td>\n",
       "      <td>-0.249176</td>\n",
       "      <td>76.100751</td>\n",
       "      <td>59.886884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic regression (smote)</td>\n",
       "      <td>0.949831</td>\n",
       "      <td>0.180735</td>\n",
       "      <td>0.869625</td>\n",
       "      <td>0.299272</td>\n",
       "      <td>-2.887710</td>\n",
       "      <td>2.010107</td>\n",
       "      <td>76.826262</td>\n",
       "      <td>61.913445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   name  accuracy  precision score  \\\n",
       "0          logistic regression (simple)  0.993811         0.877720   \n",
       "1  logistic regression (under sampling)  0.942230         0.161976   \n",
       "2   logistic regression (over sampling)  0.952178         0.188918   \n",
       "3           logistic regression (smote)  0.949831         0.180735   \n",
       "\n",
       "   recall score  f1 score  difference in accuracy (in %)  \\\n",
       "0      0.578157  0.697119                       0.013449   \n",
       "1      0.883959  0.273784                      -2.793142   \n",
       "2      0.875085  0.310750                      -3.887958   \n",
       "3      0.869625  0.299272                      -2.887710   \n",
       "\n",
       "   difference in recall (in %)  difference in precision (in %)  \\\n",
       "0                     0.022138                        0.002523   \n",
       "1                    -0.653969                       78.531292   \n",
       "2                    -0.249176                       76.100751   \n",
       "3                     2.010107                       76.826262   \n",
       "\n",
       "   difference in f1 score (in %)  \n",
       "0                       0.016887  \n",
       "1                      63.723221  \n",
       "2                      59.886884  \n",
       "3                      61.913445  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiIl9mqAoQd8"
   },
   "source": [
    "here we are encountering a problem, which can be written in points as\n",
    "1. if we do not use any sampling technique, we get a decent precision score, a bad recall score (which makes it a bad model)\n",
    "2. but if we use any sampling technique then there is a significant increase in recall but also a significant decrease in precision\n",
    "3. there is no significant difference between over sampling and smote\n",
    "4. also there is a provision for us to say that there is an observable figure denoting over fitting in case of the sampling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJABJ-G3n_QL"
   },
   "source": [
    "2. model - SUPPORT VECTOR MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "FW4DaSvMoK47"
   },
   "outputs": [],
   "source": [
    "#support_vector_machine=SVC()\n",
    "#name='support vector machine (simple)'\n",
    "#get_report(support_vector_machine,x_train,x_test,y_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "7k1RuyIXo7KC"
   },
   "outputs": [],
   "source": [
    "#name='support vector machine (under sampling)'\n",
    "#get_report(support_vector_machine,x2_train,x_test,y2_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "__YTesxxpG_2"
   },
   "outputs": [],
   "source": [
    "#name='support vector machine (over sampling)'\n",
    "#get_report(support_vector_machine,x3_train,x_test,y3_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "jyId3X6spJ1q"
   },
   "outputs": [],
   "source": [
    "#name='support vector machine (smote)'\n",
    "#get_report(support_vector_machine,x4_train,x_test,y4_train,y_test,name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1-jKLyWpUmm"
   },
   "source": [
    "3. model - DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHrHpW78pYDi",
    "outputId": "410891ab-faba-4562-b95d-9e7305cf5af2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree (simple) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    117454\n",
      "           1       0.75      0.77      0.76      1465\n",
      "\n",
      "    accuracy                           0.99    118919\n",
      "   macro avg       0.87      0.88      0.88    118919\n",
      "weighted avg       0.99      0.99      0.99    118919\n",
      "\n",
      "confusion matrix\n",
      "[[117074    380]\n",
      " [   340   1125]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.9939454586735509\n",
      "presicion score =  0.7475083056478405\n",
      "recall score = 0.7679180887372014\n",
      "F1 score = 0.7575757575757576\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 1.0\n",
      "presicion score =  1.0\n",
      "recall score = 1.0\n",
      "F1 score = 1.0\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "1.0  -  0.7575757575757576  =  0.24242424242424243\n",
      "in percentage =  24.242424242424242\n",
      "difference in recall scores\n",
      "1.0  -  0.7679180887372014  =  0.23208191126279865\n",
      "in percentage =  23.208191126279864\n",
      "difference in precision scores\n",
      "1.0  -  0.7475083056478405  =  0.25249169435215946\n",
      "in percentage =  25.249169435215947\n",
      "difference in accuracy scores\n",
      "1.0  -  0.9939454586735509  =  0.006054541326449114\n",
      "in percentage =  0.6054541326449114\n"
     ]
    }
   ],
   "source": [
    "gini = DecisionTreeClassifier(criterion = \"gini\",random_state = 100)\n",
    "\n",
    "name='decision tree (simple)'\n",
    "get_report(gini,x_train,x_test,y_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZpQYfy5pg5b",
    "outputId": "5a183709-56b6-4bd8-d2af-1077c05a6e8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree (under sampling) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98    117454\n",
      "           1       0.26      0.98      0.41      1465\n",
      "\n",
      "    accuracy                           0.97    118919\n",
      "   macro avg       0.63      0.97      0.69    118919\n",
      "weighted avg       0.99      0.97      0.97    118919\n",
      "\n",
      "confusion matrix\n",
      "[[113329   4125]\n",
      " [    36   1429]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.9650097965842296\n",
      "presicion score =  0.25729204177169607\n",
      "recall score = 0.9754266211604096\n",
      "F1 score = 0.40718051004416583\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 1.0\n",
      "presicion score =  1.0\n",
      "recall score = 1.0\n",
      "F1 score = 1.0\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "1.0  -  0.40718051004416583  =  0.5928194899558341\n",
      "in percentage =  59.28194899558341\n",
      "difference in recall scores\n",
      "1.0  -  0.9754266211604096  =  0.024573378839590432\n",
      "in percentage =  2.457337883959043\n",
      "difference in precision scores\n",
      "1.0  -  0.25729204177169607  =  0.7427079582283039\n",
      "in percentage =  74.2707958228304\n",
      "difference in accuracy scores\n",
      "1.0  -  0.9650097965842296  =  0.034990203415770416\n",
      "in percentage =  3.4990203415770416\n"
     ]
    }
   ],
   "source": [
    "name='decision tree (under sampling)'\n",
    "get_report(gini,x2_train,x_test,y2_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bd2bs_Keplew",
    "outputId": "746608ed-9333-4791-d018-5b9ea0d7e17a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree (over sampling) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    117454\n",
      "           1       0.77      0.74      0.75      1465\n",
      "\n",
      "    accuracy                           0.99    118919\n",
      "   macro avg       0.88      0.87      0.87    118919\n",
      "weighted avg       0.99      0.99      0.99    118919\n",
      "\n",
      "confusion matrix\n",
      "[[117124    330]\n",
      " [   385   1080]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.993987504099429\n",
      "presicion score =  0.7659574468085106\n",
      "recall score = 0.7372013651877133\n",
      "F1 score = 0.7513043478260869\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 1.0\n",
      "presicion score =  1.0\n",
      "recall score = 1.0\n",
      "F1 score = 1.0\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "1.0  -  0.7513043478260869  =  0.2486956521739131\n",
      "in percentage =  24.869565217391308\n",
      "difference in recall scores\n",
      "1.0  -  0.7372013651877133  =  0.2627986348122867\n",
      "in percentage =  26.27986348122867\n",
      "difference in precision scores\n",
      "1.0  -  0.7659574468085106  =  0.23404255319148937\n",
      "in percentage =  23.404255319148938\n",
      "difference in accuracy scores\n",
      "1.0  -  0.993987504099429  =  0.006012495900570958\n",
      "in percentage =  0.6012495900570958\n"
     ]
    }
   ],
   "source": [
    "name='decision tree (over sampling)'\n",
    "get_report(gini,x3_train,x_test,y3_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g20S50bZpoi6",
    "outputId": "03f25bef-111e-4025-92ef-2172f623df73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree (smote) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00    117454\n",
      "           1       0.58      0.81      0.68      1465\n",
      "\n",
      "    accuracy                           0.99    118919\n",
      "   macro avg       0.79      0.90      0.84    118919\n",
      "weighted avg       0.99      0.99      0.99    118919\n",
      "\n",
      "confusion matrix\n",
      "[[116598    856]\n",
      " [   278   1187]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.9904640974108426\n",
      "presicion score =  0.5810083210964269\n",
      "recall score = 0.8102389078498293\n",
      "F1 score = 0.676738882554162\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 1.0\n",
      "presicion score =  1.0\n",
      "recall score = 1.0\n",
      "F1 score = 1.0\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "1.0  -  0.676738882554162  =  0.32326111744583796\n",
      "in percentage =  32.3261117445838\n",
      "difference in recall scores\n",
      "1.0  -  0.8102389078498293  =  0.18976109215017067\n",
      "in percentage =  18.97610921501707\n",
      "difference in precision scores\n",
      "1.0  -  0.5810083210964269  =  0.41899167890357314\n",
      "in percentage =  41.89916789035731\n",
      "difference in accuracy scores\n",
      "1.0  -  0.9904640974108426  =  0.009535902589157375\n",
      "in percentage =  0.9535902589157375\n"
     ]
    }
   ],
   "source": [
    "name='decision tree (smote)'\n",
    "get_report(gini,x4_train,x_test,y4_train,y_test,name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3gpUFj_ptY2"
   },
   "source": [
    "4. model - RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8yaKVCC1prZe",
    "outputId": "c9b5cd61-27bd-47cb-e779-e938f568ca17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest (simple) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    117454\n",
      "           1       0.87      0.76      0.81      1465\n",
      "\n",
      "    accuracy                           1.00    118919\n",
      "   macro avg       0.94      0.88      0.91    118919\n",
      "weighted avg       1.00      1.00      1.00    118919\n",
      "\n",
      "confusion matrix\n",
      "[[117293    161]\n",
      " [   350   1115]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.9957029574752563\n",
      "presicion score =  0.8738244514106583\n",
      "recall score = 0.7610921501706485\n",
      "F1 score = 0.8135716891645385\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 0.9999957954220555\n",
      "presicion score =  1.0\n",
      "recall score = 0.9996512641673932\n",
      "F1 score = 0.9998256016742239\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "0.9998256016742239  -  0.8135716891645385  =  0.18625391250968537\n",
      "in percentage =  18.625391250968537\n",
      "difference in recall scores\n",
      "0.9996512641673932  -  0.7610921501706485  =  0.2385591139967448\n",
      "in percentage =  23.85591139967448\n",
      "difference in precision scores\n",
      "1.0  -  0.8738244514106583  =  0.12617554858934166\n",
      "in percentage =  12.617554858934167\n",
      "difference in accuracy scores\n",
      "0.9999957954220555  -  0.9957029574752563  =  0.004292837946799244\n",
      "in percentage =  0.4292837946799244\n"
     ]
    }
   ],
   "source": [
    "rfc=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "name='random forest (simple)'\n",
    "get_report(rfc,x_train,x_test,y_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T00W802UqgAd",
    "outputId": "42589b8d-b4db-4930-d6a9-810e9f8d4099"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest (under sampling) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98    117454\n",
      "           1       0.25      0.99      0.40      1465\n",
      "\n",
      "    accuracy                           0.96    118919\n",
      "   macro avg       0.62      0.98      0.69    118919\n",
      "weighted avg       0.99      0.96      0.97    118919\n",
      "\n",
      "confusion matrix\n",
      "[[113112   4342]\n",
      " [    18   1447]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.9633363886342805\n",
      "presicion score =  0.24995681464847125\n",
      "recall score = 0.9877133105802047\n",
      "F1 score = 0.3989523021781087\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 1.0\n",
      "presicion score =  1.0\n",
      "recall score = 1.0\n",
      "F1 score = 1.0\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "1.0  -  0.3989523021781087  =  0.6010476978218913\n",
      "in percentage =  60.10476978218912\n",
      "difference in recall scores\n",
      "1.0  -  0.9877133105802047  =  0.012286689419795271\n",
      "in percentage =  1.2286689419795271\n",
      "difference in precision scores\n",
      "1.0  -  0.24995681464847125  =  0.7500431853515288\n",
      "in percentage =  75.00431853515288\n",
      "difference in accuracy scores\n",
      "1.0  -  0.9633363886342805  =  0.036663611365719495\n",
      "in percentage =  3.6663611365719495\n"
     ]
    }
   ],
   "source": [
    "name='random forest (under sampling)'\n",
    "get_report(rfc,x2_train,x_test,y2_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9bJcJSBqj_i",
    "outputId": "65f26dbc-c2a9-441c-d93a-e61b612e49eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest (over sampling) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    117454\n",
      "           1       0.81      0.79      0.80      1465\n",
      "\n",
      "    accuracy                           1.00    118919\n",
      "   macro avg       0.91      0.90      0.90    118919\n",
      "weighted avg       1.00      1.00      1.00    118919\n",
      "\n",
      "confusion matrix\n",
      "[[117186    268]\n",
      " [   302   1163]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.9952068214498945\n",
      "presicion score =  0.8127183787561146\n",
      "recall score = 0.7938566552901024\n",
      "F1 score = 0.8031767955801105\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 1.0\n",
      "presicion score =  1.0\n",
      "recall score = 1.0\n",
      "F1 score = 1.0\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "1.0  -  0.8031767955801105  =  0.19682320441988954\n",
      "in percentage =  19.682320441988953\n",
      "difference in recall scores\n",
      "1.0  -  0.7938566552901024  =  0.2061433447098976\n",
      "in percentage =  20.61433447098976\n",
      "difference in precision scores\n",
      "1.0  -  0.8127183787561146  =  0.18728162124388537\n",
      "in percentage =  18.728162124388536\n",
      "difference in accuracy scores\n",
      "1.0  -  0.9952068214498945  =  0.004793178550105526\n",
      "in percentage =  0.4793178550105526\n"
     ]
    }
   ],
   "source": [
    "name='random forest (over sampling)'\n",
    "get_report(rfc,x3_train,x_test,y3_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxSyilQTqo3d",
    "outputId": "fcd58b3b-7cd2-4081-9e76-8a769324f3c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest (smote) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00    117454\n",
      "           1       0.62      0.86      0.72      1465\n",
      "\n",
      "    accuracy                           0.99    118919\n",
      "   macro avg       0.81      0.93      0.86    118919\n",
      "weighted avg       0.99      0.99      0.99    118919\n",
      "\n",
      "confusion matrix\n",
      "[[116668    786]\n",
      " [   201   1264]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.9917002329316593\n",
      "presicion score =  0.6165853658536585\n",
      "recall score = 0.8627986348122867\n",
      "F1 score = 0.7192034139402561\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 0.9999989360275952\n",
      "presicion score =  1.0\n",
      "recall score = 0.9999978720551904\n",
      "F1 score = 0.9999989360264633\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "0.9999989360264633  -  0.7192034139402561  =  0.28079552208620717\n",
      "in percentage =  28.07955220862072\n",
      "difference in recall scores\n",
      "0.9999978720551904  -  0.8627986348122867  =  0.13719923724290373\n",
      "in percentage =  13.719923724290373\n",
      "difference in precision scores\n",
      "1.0  -  0.6165853658536585  =  0.38341463414634147\n",
      "in percentage =  38.34146341463415\n",
      "difference in accuracy scores\n",
      "0.9999989360275952  -  0.9917002329316593  =  0.008298703095935833\n",
      "in percentage =  0.8298703095935833\n"
     ]
    }
   ],
   "source": [
    "name='random forest (smote)'\n",
    "get_report(rfc,x4_train,x_test,y4_train,y_test,name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5ENqc-8tBy9"
   },
   "source": [
    "5. model - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Ax_OKV1tFu5",
    "outputId": "78ac2e08-3694-4745-9eb9-af83ebda28ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn (simple) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    117454\n",
      "           1       0.82      0.61      0.70      1465\n",
      "\n",
      "    accuracy                           0.99    118919\n",
      "   macro avg       0.91      0.81      0.85    118919\n",
      "weighted avg       0.99      0.99      0.99    118919\n",
      "\n",
      "confusion matrix\n",
      "[[117261    193]\n",
      " [   568    897]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.9936006861813503\n",
      "presicion score =  0.8229357798165138\n",
      "recall score = 0.6122866894197952\n",
      "F1 score = 0.7021526418786693\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 0.9947379707025009\n",
      "presicion score =  0.8747680890538033\n",
      "recall score = 0.6577157802964254\n",
      "F1 score = 0.7508709067383299\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "0.7508709067383299  -  0.7021526418786693  =  0.048718264859660576\n",
      "in percentage =  4.871826485966057\n",
      "difference in recall scores\n",
      "0.6577157802964254  -  0.6122866894197952  =  0.04542909087663016\n",
      "in percentage =  4.542909087663016\n",
      "difference in precision scores\n",
      "0.8747680890538033  -  0.8229357798165138  =  0.05183230923728954\n",
      "in percentage =  5.183230923728955\n",
      "difference in accuracy scores\n",
      "0.9947379707025009  -  0.9936006861813503  =  0.0011372845211505833\n",
      "in percentage =  0.11372845211505833\n"
     ]
    }
   ],
   "source": [
    "knn=KNeighborsClassifier()\n",
    "\n",
    "name = 'knn (simple)'\n",
    "get_report(knn,x_train,x_test,y_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jU9XMZKtL6g",
    "outputId": "ae0d2639-ff17-45f1-d6ed-a5eaece67538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn (under sampling) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98    117454\n",
      "           1       0.19      0.89      0.32      1465\n",
      "\n",
      "    accuracy                           0.95    118919\n",
      "   macro avg       0.60      0.92      0.65    118919\n",
      "weighted avg       0.99      0.95      0.97    118919\n",
      "\n",
      "confusion matrix\n",
      "[[111952   5502]\n",
      " [   157   1308]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.9524129869911452\n",
      "presicion score =  0.1920704845814978\n",
      "recall score = 0.8928327645051195\n",
      "F1 score = 0.3161329305135952\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 0.9391455972101134\n",
      "presicion score =  0.9649252353701311\n",
      "recall score = 0.9114210985178727\n",
      "F1 score = 0.9374103299856528\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "0.9374103299856528  -  0.3161329305135952  =  0.6212773994720575\n",
      "in percentage =  62.127739947205754\n",
      "difference in recall scores\n",
      "0.9114210985178727  -  0.8928327645051195  =  0.01858833401275317\n",
      "in percentage =  1.8588334012753172\n",
      "difference in precision scores\n",
      "0.9649252353701311  -  0.1920704845814978  =  0.7728547507886333\n",
      "in percentage =  77.28547507886333\n",
      "difference in accuracy scores\n",
      "0.9391455972101134  -  0.9524129869911452  =  -0.013267389781031813\n",
      "in percentage =  -1.3267389781031813\n"
     ]
    }
   ],
   "source": [
    "name = 'knn (under sampling)'\n",
    "get_report(knn,x2_train,x_test,y2_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L7QSSnjCtSo7",
    "outputId": "370e8857-dadf-4bd3-fc74-5b3fdc804f87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn (over sampling) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    117454\n",
      "           1       0.50      0.75      0.60      1465\n",
      "\n",
      "    accuracy                           0.99    118919\n",
      "   macro avg       0.75      0.87      0.80    118919\n",
      "weighted avg       0.99      0.99      0.99    118919\n",
      "\n",
      "confusion matrix\n",
      "[[116349   1105]\n",
      " [   362   1103]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.98766387204736\n",
      "presicion score =  0.4995471014492754\n",
      "recall score = 0.752901023890785\n",
      "F1 score = 0.6005989654233597\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 0.9966410391180094\n",
      "presicion score =  0.9933269075490283\n",
      "recall score = 1.0\n",
      "F1 score = 0.9966522839652143\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "0.9966522839652143  -  0.6005989654233597  =  0.39605331854185466\n",
      "in percentage =  39.605331854185465\n",
      "difference in recall scores\n",
      "1.0  -  0.752901023890785  =  0.24709897610921505\n",
      "in percentage =  24.709897610921505\n",
      "difference in precision scores\n",
      "0.9933269075490283  -  0.4995471014492754  =  0.4937798060997529\n",
      "in percentage =  49.37798060997529\n",
      "difference in accuracy scores\n",
      "0.9966410391180094  -  0.98766387204736  =  0.008977167070649439\n",
      "in percentage =  0.8977167070649439\n"
     ]
    }
   ],
   "source": [
    "name = 'knn (over sampling)'\n",
    "get_report(knn,x3_train,x_test,y3_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ElYMvfptZ5L",
    "outputId": "ad96059d-5391-49b4-8434-42ed9e885b25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn (smote) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99    117454\n",
      "           1       0.32      0.84      0.46      1465\n",
      "\n",
      "    accuracy                           0.98    118919\n",
      "   macro avg       0.66      0.91      0.72    118919\n",
      "weighted avg       0.99      0.98      0.98    118919\n",
      "\n",
      "confusion matrix\n",
      "[[114806   2648]\n",
      " [   237   1228]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.9757397892683255\n",
      "presicion score =  0.31682146542827655\n",
      "recall score = 0.8382252559726963\n",
      "F1 score = 0.4598389814641453\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 0.9908689888219059\n",
      "presicion score =  0.982838412317662\n",
      "recall score = 0.9991849971379142\n",
      "F1 score = 0.9909442961305912\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "0.9909442961305912  -  0.4598389814641453  =  0.531105314666446\n",
      "in percentage =  53.110531466644595\n",
      "difference in recall scores\n",
      "0.9991849971379142  -  0.8382252559726963  =  0.16095974116521794\n",
      "in percentage =  16.095974116521795\n",
      "difference in precision scores\n",
      "0.982838412317662  -  0.31682146542827655  =  0.6660169468893855\n",
      "in percentage =  66.60169468893855\n",
      "difference in accuracy scores\n",
      "0.9908689888219059  -  0.9757397892683255  =  0.015129199553580386\n",
      "in percentage =  1.5129199553580386\n"
     ]
    }
   ],
   "source": [
    "name = 'knn (smote)'\n",
    "get_report(knn,x4_train,x_test,y4_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient boosting \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    117454\n",
      "           1       0.91      0.70      0.79      1465\n",
      "\n",
      "    accuracy                           1.00    118919\n",
      "   macro avg       0.95      0.85      0.90    118919\n",
      "weighted avg       1.00      1.00      1.00    118919\n",
      "\n",
      "confusion matrix\n",
      "[[117354    100]\n",
      " [   437   1028]] \n",
      "\n",
      "for test data\n",
      "accuracy = 0.9954843212606901\n",
      "presicion score =  0.9113475177304965\n",
      "recall score = 0.7017064846416382\n",
      "F1 score = 0.7929039722329349\n",
      "\n",
      "\n",
      "for train data\n",
      "accuracy = 0.9957449671202004\n",
      "presicion score =  0.9178113037604143\n",
      "recall score = 0.7107236268526591\n",
      "F1 score = 0.8011006289308177\n",
      "\n",
      "\n",
      "to understand whether our model is overfitting or underfitting\n",
      "difference in f1 scores\n",
      "0.8011006289308177  -  0.7929039722329349  =  0.008196656697882831\n",
      "in percentage =  0.8196656697882831\n",
      "difference in recall scores\n",
      "0.7107236268526591  -  0.7017064846416382  =  0.009017142211020901\n",
      "in percentage =  0.9017142211020901\n",
      "difference in precision scores\n",
      "0.9178113037604143  -  0.9113475177304965  =  0.006463786029917817\n",
      "in percentage =  0.6463786029917817\n",
      "difference in accuracy scores\n",
      "0.9957449671202004  -  0.9954843212606901  =  0.00026064585951035557\n",
      "in percentage =  0.026064585951035557\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "name = 'Gradient boosting'\n",
    "gbm=GradientBoostingClassifier()\n",
    "get_report(gbm,x_train,x_test,y_train,y_test,name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "snMtKEG9smVT",
    "outputId": "032247d0-8814-40ad-8100-07462420724a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision score</th>\n",
       "      <th>recall score</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>difference in accuracy (in %)</th>\n",
       "      <th>difference in recall (in %)</th>\n",
       "      <th>difference in precision (in %)</th>\n",
       "      <th>difference in f1 score (in %)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic regression (simple)</td>\n",
       "      <td>0.993811</td>\n",
       "      <td>0.877720</td>\n",
       "      <td>0.578157</td>\n",
       "      <td>0.697119</td>\n",
       "      <td>0.013449</td>\n",
       "      <td>0.022138</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.016887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic regression (under sampling)</td>\n",
       "      <td>0.942230</td>\n",
       "      <td>0.161976</td>\n",
       "      <td>0.883959</td>\n",
       "      <td>0.273784</td>\n",
       "      <td>-2.793142</td>\n",
       "      <td>-0.653969</td>\n",
       "      <td>78.531292</td>\n",
       "      <td>63.723221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic regression (over sampling)</td>\n",
       "      <td>0.952178</td>\n",
       "      <td>0.188918</td>\n",
       "      <td>0.875085</td>\n",
       "      <td>0.310750</td>\n",
       "      <td>-3.887958</td>\n",
       "      <td>-0.249176</td>\n",
       "      <td>76.100751</td>\n",
       "      <td>59.886884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic regression (smote)</td>\n",
       "      <td>0.949831</td>\n",
       "      <td>0.180735</td>\n",
       "      <td>0.869625</td>\n",
       "      <td>0.299272</td>\n",
       "      <td>-2.887710</td>\n",
       "      <td>2.010107</td>\n",
       "      <td>76.826262</td>\n",
       "      <td>61.913445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>decision tree (simple)</td>\n",
       "      <td>0.993945</td>\n",
       "      <td>0.747508</td>\n",
       "      <td>0.767918</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.605454</td>\n",
       "      <td>23.208191</td>\n",
       "      <td>25.249169</td>\n",
       "      <td>24.242424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>decision tree (under sampling)</td>\n",
       "      <td>0.965010</td>\n",
       "      <td>0.257292</td>\n",
       "      <td>0.975427</td>\n",
       "      <td>0.407181</td>\n",
       "      <td>3.499020</td>\n",
       "      <td>2.457338</td>\n",
       "      <td>74.270796</td>\n",
       "      <td>59.281949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>decision tree (over sampling)</td>\n",
       "      <td>0.993988</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.737201</td>\n",
       "      <td>0.751304</td>\n",
       "      <td>0.601250</td>\n",
       "      <td>26.279863</td>\n",
       "      <td>23.404255</td>\n",
       "      <td>24.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>decision tree (smote)</td>\n",
       "      <td>0.990464</td>\n",
       "      <td>0.581008</td>\n",
       "      <td>0.810239</td>\n",
       "      <td>0.676739</td>\n",
       "      <td>0.953590</td>\n",
       "      <td>18.976109</td>\n",
       "      <td>41.899168</td>\n",
       "      <td>32.326112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>random forest (simple)</td>\n",
       "      <td>0.995703</td>\n",
       "      <td>0.873824</td>\n",
       "      <td>0.761092</td>\n",
       "      <td>0.813572</td>\n",
       "      <td>0.429284</td>\n",
       "      <td>23.855911</td>\n",
       "      <td>12.617555</td>\n",
       "      <td>18.625391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>random forest (under sampling)</td>\n",
       "      <td>0.963336</td>\n",
       "      <td>0.249957</td>\n",
       "      <td>0.987713</td>\n",
       "      <td>0.398952</td>\n",
       "      <td>3.666361</td>\n",
       "      <td>1.228669</td>\n",
       "      <td>75.004319</td>\n",
       "      <td>60.104770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>random forest (over sampling)</td>\n",
       "      <td>0.995207</td>\n",
       "      <td>0.812718</td>\n",
       "      <td>0.793857</td>\n",
       "      <td>0.803177</td>\n",
       "      <td>0.479318</td>\n",
       "      <td>20.614334</td>\n",
       "      <td>18.728162</td>\n",
       "      <td>19.682320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>random forest (smote)</td>\n",
       "      <td>0.991700</td>\n",
       "      <td>0.616585</td>\n",
       "      <td>0.862799</td>\n",
       "      <td>0.719203</td>\n",
       "      <td>0.829870</td>\n",
       "      <td>13.719924</td>\n",
       "      <td>38.341463</td>\n",
       "      <td>28.079552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>knn (simple)</td>\n",
       "      <td>0.993601</td>\n",
       "      <td>0.822936</td>\n",
       "      <td>0.612287</td>\n",
       "      <td>0.702153</td>\n",
       "      <td>0.113728</td>\n",
       "      <td>4.542909</td>\n",
       "      <td>5.183231</td>\n",
       "      <td>4.871826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>knn (under sampling)</td>\n",
       "      <td>0.952413</td>\n",
       "      <td>0.192070</td>\n",
       "      <td>0.892833</td>\n",
       "      <td>0.316133</td>\n",
       "      <td>-1.326739</td>\n",
       "      <td>1.858833</td>\n",
       "      <td>77.285475</td>\n",
       "      <td>62.127740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>knn (over sampling)</td>\n",
       "      <td>0.987664</td>\n",
       "      <td>0.499547</td>\n",
       "      <td>0.752901</td>\n",
       "      <td>0.600599</td>\n",
       "      <td>0.897717</td>\n",
       "      <td>24.709898</td>\n",
       "      <td>49.377981</td>\n",
       "      <td>39.605332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>knn (smote)</td>\n",
       "      <td>0.975740</td>\n",
       "      <td>0.316821</td>\n",
       "      <td>0.838225</td>\n",
       "      <td>0.459839</td>\n",
       "      <td>1.512920</td>\n",
       "      <td>16.095974</td>\n",
       "      <td>66.601695</td>\n",
       "      <td>53.110531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Gradient boosting</td>\n",
       "      <td>0.995484</td>\n",
       "      <td>0.911348</td>\n",
       "      <td>0.701706</td>\n",
       "      <td>0.792904</td>\n",
       "      <td>0.026065</td>\n",
       "      <td>0.901714</td>\n",
       "      <td>0.646379</td>\n",
       "      <td>0.819666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    name  accuracy  precision score  \\\n",
       "0           logistic regression (simple)  0.993811         0.877720   \n",
       "1   logistic regression (under sampling)  0.942230         0.161976   \n",
       "2    logistic regression (over sampling)  0.952178         0.188918   \n",
       "3            logistic regression (smote)  0.949831         0.180735   \n",
       "8                 decision tree (simple)  0.993945         0.747508   \n",
       "9         decision tree (under sampling)  0.965010         0.257292   \n",
       "10         decision tree (over sampling)  0.993988         0.765957   \n",
       "11                 decision tree (smote)  0.990464         0.581008   \n",
       "12                random forest (simple)  0.995703         0.873824   \n",
       "13        random forest (under sampling)  0.963336         0.249957   \n",
       "14         random forest (over sampling)  0.995207         0.812718   \n",
       "15                 random forest (smote)  0.991700         0.616585   \n",
       "20                          knn (simple)  0.993601         0.822936   \n",
       "21                  knn (under sampling)  0.952413         0.192070   \n",
       "22                   knn (over sampling)  0.987664         0.499547   \n",
       "23                           knn (smote)  0.975740         0.316821   \n",
       "24                     Gradient boosting  0.995484         0.911348   \n",
       "\n",
       "    recall score  f1 score  difference in accuracy (in %)  \\\n",
       "0       0.578157  0.697119                       0.013449   \n",
       "1       0.883959  0.273784                      -2.793142   \n",
       "2       0.875085  0.310750                      -3.887958   \n",
       "3       0.869625  0.299272                      -2.887710   \n",
       "8       0.767918  0.757576                       0.605454   \n",
       "9       0.975427  0.407181                       3.499020   \n",
       "10      0.737201  0.751304                       0.601250   \n",
       "11      0.810239  0.676739                       0.953590   \n",
       "12      0.761092  0.813572                       0.429284   \n",
       "13      0.987713  0.398952                       3.666361   \n",
       "14      0.793857  0.803177                       0.479318   \n",
       "15      0.862799  0.719203                       0.829870   \n",
       "20      0.612287  0.702153                       0.113728   \n",
       "21      0.892833  0.316133                      -1.326739   \n",
       "22      0.752901  0.600599                       0.897717   \n",
       "23      0.838225  0.459839                       1.512920   \n",
       "24      0.701706  0.792904                       0.026065   \n",
       "\n",
       "    difference in recall (in %)  difference in precision (in %)  \\\n",
       "0                      0.022138                        0.002523   \n",
       "1                     -0.653969                       78.531292   \n",
       "2                     -0.249176                       76.100751   \n",
       "3                      2.010107                       76.826262   \n",
       "8                     23.208191                       25.249169   \n",
       "9                      2.457338                       74.270796   \n",
       "10                    26.279863                       23.404255   \n",
       "11                    18.976109                       41.899168   \n",
       "12                    23.855911                       12.617555   \n",
       "13                     1.228669                       75.004319   \n",
       "14                    20.614334                       18.728162   \n",
       "15                    13.719924                       38.341463   \n",
       "20                     4.542909                        5.183231   \n",
       "21                     1.858833                       77.285475   \n",
       "22                    24.709898                       49.377981   \n",
       "23                    16.095974                       66.601695   \n",
       "24                     0.901714                        0.646379   \n",
       "\n",
       "    difference in f1 score (in %)  \n",
       "0                        0.016887  \n",
       "1                       63.723221  \n",
       "2                       59.886884  \n",
       "3                       61.913445  \n",
       "8                       24.242424  \n",
       "9                       59.281949  \n",
       "10                      24.869565  \n",
       "11                      32.326112  \n",
       "12                      18.625391  \n",
       "13                      60.104770  \n",
       "14                      19.682320  \n",
       "15                      28.079552  \n",
       "20                       4.871826  \n",
       "21                      62.127740  \n",
       "22                      39.605332  \n",
       "23                      53.110531  \n",
       "24                       0.819666  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_used = models.dropna()\n",
    "models_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "RhKhSWGOPvl8",
    "outputId": "9eb53a36-1b6d-471a-853c-8fdc5ccda9e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision score</th>\n",
       "      <th>recall score</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>difference in accuracy (in %)</th>\n",
       "      <th>difference in recall (in %)</th>\n",
       "      <th>difference in precision (in %)</th>\n",
       "      <th>difference in f1 score (in %)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>random forest (simple)</td>\n",
       "      <td>0.995703</td>\n",
       "      <td>0.873824</td>\n",
       "      <td>0.761092</td>\n",
       "      <td>0.813572</td>\n",
       "      <td>0.429284</td>\n",
       "      <td>23.855911</td>\n",
       "      <td>12.617555</td>\n",
       "      <td>18.625391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>random forest (over sampling)</td>\n",
       "      <td>0.995207</td>\n",
       "      <td>0.812718</td>\n",
       "      <td>0.793857</td>\n",
       "      <td>0.803177</td>\n",
       "      <td>0.479318</td>\n",
       "      <td>20.614334</td>\n",
       "      <td>18.728162</td>\n",
       "      <td>19.682320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Gradient boosting</td>\n",
       "      <td>0.995484</td>\n",
       "      <td>0.911348</td>\n",
       "      <td>0.701706</td>\n",
       "      <td>0.792904</td>\n",
       "      <td>0.026065</td>\n",
       "      <td>0.901714</td>\n",
       "      <td>0.646379</td>\n",
       "      <td>0.819666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>decision tree (simple)</td>\n",
       "      <td>0.993945</td>\n",
       "      <td>0.747508</td>\n",
       "      <td>0.767918</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.605454</td>\n",
       "      <td>23.208191</td>\n",
       "      <td>25.249169</td>\n",
       "      <td>24.242424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>decision tree (over sampling)</td>\n",
       "      <td>0.993988</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.737201</td>\n",
       "      <td>0.751304</td>\n",
       "      <td>0.601250</td>\n",
       "      <td>26.279863</td>\n",
       "      <td>23.404255</td>\n",
       "      <td>24.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>random forest (smote)</td>\n",
       "      <td>0.991700</td>\n",
       "      <td>0.616585</td>\n",
       "      <td>0.862799</td>\n",
       "      <td>0.719203</td>\n",
       "      <td>0.829870</td>\n",
       "      <td>13.719924</td>\n",
       "      <td>38.341463</td>\n",
       "      <td>28.079552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>knn (simple)</td>\n",
       "      <td>0.993601</td>\n",
       "      <td>0.822936</td>\n",
       "      <td>0.612287</td>\n",
       "      <td>0.702153</td>\n",
       "      <td>0.113728</td>\n",
       "      <td>4.542909</td>\n",
       "      <td>5.183231</td>\n",
       "      <td>4.871826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic regression (simple)</td>\n",
       "      <td>0.993811</td>\n",
       "      <td>0.877720</td>\n",
       "      <td>0.578157</td>\n",
       "      <td>0.697119</td>\n",
       "      <td>0.013449</td>\n",
       "      <td>0.022138</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.016887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>decision tree (smote)</td>\n",
       "      <td>0.990464</td>\n",
       "      <td>0.581008</td>\n",
       "      <td>0.810239</td>\n",
       "      <td>0.676739</td>\n",
       "      <td>0.953590</td>\n",
       "      <td>18.976109</td>\n",
       "      <td>41.899168</td>\n",
       "      <td>32.326112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>knn (over sampling)</td>\n",
       "      <td>0.987664</td>\n",
       "      <td>0.499547</td>\n",
       "      <td>0.752901</td>\n",
       "      <td>0.600599</td>\n",
       "      <td>0.897717</td>\n",
       "      <td>24.709898</td>\n",
       "      <td>49.377981</td>\n",
       "      <td>39.605332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>knn (smote)</td>\n",
       "      <td>0.975740</td>\n",
       "      <td>0.316821</td>\n",
       "      <td>0.838225</td>\n",
       "      <td>0.459839</td>\n",
       "      <td>1.512920</td>\n",
       "      <td>16.095974</td>\n",
       "      <td>66.601695</td>\n",
       "      <td>53.110531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>decision tree (under sampling)</td>\n",
       "      <td>0.965010</td>\n",
       "      <td>0.257292</td>\n",
       "      <td>0.975427</td>\n",
       "      <td>0.407181</td>\n",
       "      <td>3.499020</td>\n",
       "      <td>2.457338</td>\n",
       "      <td>74.270796</td>\n",
       "      <td>59.281949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>random forest (under sampling)</td>\n",
       "      <td>0.963336</td>\n",
       "      <td>0.249957</td>\n",
       "      <td>0.987713</td>\n",
       "      <td>0.398952</td>\n",
       "      <td>3.666361</td>\n",
       "      <td>1.228669</td>\n",
       "      <td>75.004319</td>\n",
       "      <td>60.104770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>knn (under sampling)</td>\n",
       "      <td>0.952413</td>\n",
       "      <td>0.192070</td>\n",
       "      <td>0.892833</td>\n",
       "      <td>0.316133</td>\n",
       "      <td>-1.326739</td>\n",
       "      <td>1.858833</td>\n",
       "      <td>77.285475</td>\n",
       "      <td>62.127740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic regression (over sampling)</td>\n",
       "      <td>0.952178</td>\n",
       "      <td>0.188918</td>\n",
       "      <td>0.875085</td>\n",
       "      <td>0.310750</td>\n",
       "      <td>-3.887958</td>\n",
       "      <td>-0.249176</td>\n",
       "      <td>76.100751</td>\n",
       "      <td>59.886884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic regression (smote)</td>\n",
       "      <td>0.949831</td>\n",
       "      <td>0.180735</td>\n",
       "      <td>0.869625</td>\n",
       "      <td>0.299272</td>\n",
       "      <td>-2.887710</td>\n",
       "      <td>2.010107</td>\n",
       "      <td>76.826262</td>\n",
       "      <td>61.913445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic regression (under sampling)</td>\n",
       "      <td>0.942230</td>\n",
       "      <td>0.161976</td>\n",
       "      <td>0.883959</td>\n",
       "      <td>0.273784</td>\n",
       "      <td>-2.793142</td>\n",
       "      <td>-0.653969</td>\n",
       "      <td>78.531292</td>\n",
       "      <td>63.723221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    name  accuracy  precision score  \\\n",
       "12                random forest (simple)  0.995703         0.873824   \n",
       "14         random forest (over sampling)  0.995207         0.812718   \n",
       "24                     Gradient boosting  0.995484         0.911348   \n",
       "8                 decision tree (simple)  0.993945         0.747508   \n",
       "10         decision tree (over sampling)  0.993988         0.765957   \n",
       "15                 random forest (smote)  0.991700         0.616585   \n",
       "20                          knn (simple)  0.993601         0.822936   \n",
       "0           logistic regression (simple)  0.993811         0.877720   \n",
       "11                 decision tree (smote)  0.990464         0.581008   \n",
       "22                   knn (over sampling)  0.987664         0.499547   \n",
       "23                           knn (smote)  0.975740         0.316821   \n",
       "9         decision tree (under sampling)  0.965010         0.257292   \n",
       "13        random forest (under sampling)  0.963336         0.249957   \n",
       "21                  knn (under sampling)  0.952413         0.192070   \n",
       "2    logistic regression (over sampling)  0.952178         0.188918   \n",
       "3            logistic regression (smote)  0.949831         0.180735   \n",
       "1   logistic regression (under sampling)  0.942230         0.161976   \n",
       "\n",
       "    recall score  f1 score  difference in accuracy (in %)  \\\n",
       "12      0.761092  0.813572                       0.429284   \n",
       "14      0.793857  0.803177                       0.479318   \n",
       "24      0.701706  0.792904                       0.026065   \n",
       "8       0.767918  0.757576                       0.605454   \n",
       "10      0.737201  0.751304                       0.601250   \n",
       "15      0.862799  0.719203                       0.829870   \n",
       "20      0.612287  0.702153                       0.113728   \n",
       "0       0.578157  0.697119                       0.013449   \n",
       "11      0.810239  0.676739                       0.953590   \n",
       "22      0.752901  0.600599                       0.897717   \n",
       "23      0.838225  0.459839                       1.512920   \n",
       "9       0.975427  0.407181                       3.499020   \n",
       "13      0.987713  0.398952                       3.666361   \n",
       "21      0.892833  0.316133                      -1.326739   \n",
       "2       0.875085  0.310750                      -3.887958   \n",
       "3       0.869625  0.299272                      -2.887710   \n",
       "1       0.883959  0.273784                      -2.793142   \n",
       "\n",
       "    difference in recall (in %)  difference in precision (in %)  \\\n",
       "12                    23.855911                       12.617555   \n",
       "14                    20.614334                       18.728162   \n",
       "24                     0.901714                        0.646379   \n",
       "8                     23.208191                       25.249169   \n",
       "10                    26.279863                       23.404255   \n",
       "15                    13.719924                       38.341463   \n",
       "20                     4.542909                        5.183231   \n",
       "0                      0.022138                        0.002523   \n",
       "11                    18.976109                       41.899168   \n",
       "22                    24.709898                       49.377981   \n",
       "23                    16.095974                       66.601695   \n",
       "9                      2.457338                       74.270796   \n",
       "13                     1.228669                       75.004319   \n",
       "21                     1.858833                       77.285475   \n",
       "2                     -0.249176                       76.100751   \n",
       "3                      2.010107                       76.826262   \n",
       "1                     -0.653969                       78.531292   \n",
       "\n",
       "    difference in f1 score (in %)  \n",
       "12                      18.625391  \n",
       "14                      19.682320  \n",
       "24                       0.819666  \n",
       "8                       24.242424  \n",
       "10                      24.869565  \n",
       "15                      28.079552  \n",
       "20                       4.871826  \n",
       "0                        0.016887  \n",
       "11                      32.326112  \n",
       "22                      39.605332  \n",
       "23                      53.110531  \n",
       "9                       59.281949  \n",
       "13                      60.104770  \n",
       "21                      62.127740  \n",
       "2                       59.886884  \n",
       "3                       61.913445  \n",
       "1                       63.723221  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_used.sort_values(by='f1 score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "Rv-PZmYvRW7y",
    "outputId": "e5015780-4417-470a-fa06-c0271f6e14cb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision score</th>\n",
       "      <th>recall score</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>difference in accuracy (in %)</th>\n",
       "      <th>difference in recall (in %)</th>\n",
       "      <th>difference in precision (in %)</th>\n",
       "      <th>difference in f1 score (in %)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>random forest (under sampling)</td>\n",
       "      <td>0.963336</td>\n",
       "      <td>0.249957</td>\n",
       "      <td>0.987713</td>\n",
       "      <td>0.398952</td>\n",
       "      <td>3.666361</td>\n",
       "      <td>1.228669</td>\n",
       "      <td>75.004319</td>\n",
       "      <td>60.104770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>decision tree (under sampling)</td>\n",
       "      <td>0.965010</td>\n",
       "      <td>0.257292</td>\n",
       "      <td>0.975427</td>\n",
       "      <td>0.407181</td>\n",
       "      <td>3.499020</td>\n",
       "      <td>2.457338</td>\n",
       "      <td>74.270796</td>\n",
       "      <td>59.281949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>knn (under sampling)</td>\n",
       "      <td>0.952413</td>\n",
       "      <td>0.192070</td>\n",
       "      <td>0.892833</td>\n",
       "      <td>0.316133</td>\n",
       "      <td>-1.326739</td>\n",
       "      <td>1.858833</td>\n",
       "      <td>77.285475</td>\n",
       "      <td>62.127740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic regression (under sampling)</td>\n",
       "      <td>0.942230</td>\n",
       "      <td>0.161976</td>\n",
       "      <td>0.883959</td>\n",
       "      <td>0.273784</td>\n",
       "      <td>-2.793142</td>\n",
       "      <td>-0.653969</td>\n",
       "      <td>78.531292</td>\n",
       "      <td>63.723221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic regression (over sampling)</td>\n",
       "      <td>0.952178</td>\n",
       "      <td>0.188918</td>\n",
       "      <td>0.875085</td>\n",
       "      <td>0.310750</td>\n",
       "      <td>-3.887958</td>\n",
       "      <td>-0.249176</td>\n",
       "      <td>76.100751</td>\n",
       "      <td>59.886884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic regression (smote)</td>\n",
       "      <td>0.949831</td>\n",
       "      <td>0.180735</td>\n",
       "      <td>0.869625</td>\n",
       "      <td>0.299272</td>\n",
       "      <td>-2.887710</td>\n",
       "      <td>2.010107</td>\n",
       "      <td>76.826262</td>\n",
       "      <td>61.913445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>random forest (smote)</td>\n",
       "      <td>0.991700</td>\n",
       "      <td>0.616585</td>\n",
       "      <td>0.862799</td>\n",
       "      <td>0.719203</td>\n",
       "      <td>0.829870</td>\n",
       "      <td>13.719924</td>\n",
       "      <td>38.341463</td>\n",
       "      <td>28.079552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>knn (smote)</td>\n",
       "      <td>0.975740</td>\n",
       "      <td>0.316821</td>\n",
       "      <td>0.838225</td>\n",
       "      <td>0.459839</td>\n",
       "      <td>1.512920</td>\n",
       "      <td>16.095974</td>\n",
       "      <td>66.601695</td>\n",
       "      <td>53.110531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>decision tree (smote)</td>\n",
       "      <td>0.990464</td>\n",
       "      <td>0.581008</td>\n",
       "      <td>0.810239</td>\n",
       "      <td>0.676739</td>\n",
       "      <td>0.953590</td>\n",
       "      <td>18.976109</td>\n",
       "      <td>41.899168</td>\n",
       "      <td>32.326112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>random forest (over sampling)</td>\n",
       "      <td>0.995207</td>\n",
       "      <td>0.812718</td>\n",
       "      <td>0.793857</td>\n",
       "      <td>0.803177</td>\n",
       "      <td>0.479318</td>\n",
       "      <td>20.614334</td>\n",
       "      <td>18.728162</td>\n",
       "      <td>19.682320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>decision tree (simple)</td>\n",
       "      <td>0.993945</td>\n",
       "      <td>0.747508</td>\n",
       "      <td>0.767918</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.605454</td>\n",
       "      <td>23.208191</td>\n",
       "      <td>25.249169</td>\n",
       "      <td>24.242424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>random forest (simple)</td>\n",
       "      <td>0.995703</td>\n",
       "      <td>0.873824</td>\n",
       "      <td>0.761092</td>\n",
       "      <td>0.813572</td>\n",
       "      <td>0.429284</td>\n",
       "      <td>23.855911</td>\n",
       "      <td>12.617555</td>\n",
       "      <td>18.625391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>knn (over sampling)</td>\n",
       "      <td>0.987664</td>\n",
       "      <td>0.499547</td>\n",
       "      <td>0.752901</td>\n",
       "      <td>0.600599</td>\n",
       "      <td>0.897717</td>\n",
       "      <td>24.709898</td>\n",
       "      <td>49.377981</td>\n",
       "      <td>39.605332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>decision tree (over sampling)</td>\n",
       "      <td>0.993988</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.737201</td>\n",
       "      <td>0.751304</td>\n",
       "      <td>0.601250</td>\n",
       "      <td>26.279863</td>\n",
       "      <td>23.404255</td>\n",
       "      <td>24.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Gradient boosting</td>\n",
       "      <td>0.995484</td>\n",
       "      <td>0.911348</td>\n",
       "      <td>0.701706</td>\n",
       "      <td>0.792904</td>\n",
       "      <td>0.026065</td>\n",
       "      <td>0.901714</td>\n",
       "      <td>0.646379</td>\n",
       "      <td>0.819666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>knn (simple)</td>\n",
       "      <td>0.993601</td>\n",
       "      <td>0.822936</td>\n",
       "      <td>0.612287</td>\n",
       "      <td>0.702153</td>\n",
       "      <td>0.113728</td>\n",
       "      <td>4.542909</td>\n",
       "      <td>5.183231</td>\n",
       "      <td>4.871826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic regression (simple)</td>\n",
       "      <td>0.993811</td>\n",
       "      <td>0.877720</td>\n",
       "      <td>0.578157</td>\n",
       "      <td>0.697119</td>\n",
       "      <td>0.013449</td>\n",
       "      <td>0.022138</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.016887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    name  accuracy  precision score  \\\n",
       "13        random forest (under sampling)  0.963336         0.249957   \n",
       "9         decision tree (under sampling)  0.965010         0.257292   \n",
       "21                  knn (under sampling)  0.952413         0.192070   \n",
       "1   logistic regression (under sampling)  0.942230         0.161976   \n",
       "2    logistic regression (over sampling)  0.952178         0.188918   \n",
       "3            logistic regression (smote)  0.949831         0.180735   \n",
       "15                 random forest (smote)  0.991700         0.616585   \n",
       "23                           knn (smote)  0.975740         0.316821   \n",
       "11                 decision tree (smote)  0.990464         0.581008   \n",
       "14         random forest (over sampling)  0.995207         0.812718   \n",
       "8                 decision tree (simple)  0.993945         0.747508   \n",
       "12                random forest (simple)  0.995703         0.873824   \n",
       "22                   knn (over sampling)  0.987664         0.499547   \n",
       "10         decision tree (over sampling)  0.993988         0.765957   \n",
       "24                     Gradient boosting  0.995484         0.911348   \n",
       "20                          knn (simple)  0.993601         0.822936   \n",
       "0           logistic regression (simple)  0.993811         0.877720   \n",
       "\n",
       "    recall score  f1 score  difference in accuracy (in %)  \\\n",
       "13      0.987713  0.398952                       3.666361   \n",
       "9       0.975427  0.407181                       3.499020   \n",
       "21      0.892833  0.316133                      -1.326739   \n",
       "1       0.883959  0.273784                      -2.793142   \n",
       "2       0.875085  0.310750                      -3.887958   \n",
       "3       0.869625  0.299272                      -2.887710   \n",
       "15      0.862799  0.719203                       0.829870   \n",
       "23      0.838225  0.459839                       1.512920   \n",
       "11      0.810239  0.676739                       0.953590   \n",
       "14      0.793857  0.803177                       0.479318   \n",
       "8       0.767918  0.757576                       0.605454   \n",
       "12      0.761092  0.813572                       0.429284   \n",
       "22      0.752901  0.600599                       0.897717   \n",
       "10      0.737201  0.751304                       0.601250   \n",
       "24      0.701706  0.792904                       0.026065   \n",
       "20      0.612287  0.702153                       0.113728   \n",
       "0       0.578157  0.697119                       0.013449   \n",
       "\n",
       "    difference in recall (in %)  difference in precision (in %)  \\\n",
       "13                     1.228669                       75.004319   \n",
       "9                      2.457338                       74.270796   \n",
       "21                     1.858833                       77.285475   \n",
       "1                     -0.653969                       78.531292   \n",
       "2                     -0.249176                       76.100751   \n",
       "3                      2.010107                       76.826262   \n",
       "15                    13.719924                       38.341463   \n",
       "23                    16.095974                       66.601695   \n",
       "11                    18.976109                       41.899168   \n",
       "14                    20.614334                       18.728162   \n",
       "8                     23.208191                       25.249169   \n",
       "12                    23.855911                       12.617555   \n",
       "22                    24.709898                       49.377981   \n",
       "10                    26.279863                       23.404255   \n",
       "24                     0.901714                        0.646379   \n",
       "20                     4.542909                        5.183231   \n",
       "0                      0.022138                        0.002523   \n",
       "\n",
       "    difference in f1 score (in %)  \n",
       "13                      60.104770  \n",
       "9                       59.281949  \n",
       "21                      62.127740  \n",
       "1                       63.723221  \n",
       "2                       59.886884  \n",
       "3                       61.913445  \n",
       "15                      28.079552  \n",
       "23                      53.110531  \n",
       "11                      32.326112  \n",
       "14                      19.682320  \n",
       "8                       24.242424  \n",
       "12                      18.625391  \n",
       "22                      39.605332  \n",
       "10                      24.869565  \n",
       "24                       0.819666  \n",
       "20                       4.871826  \n",
       "0                        0.016887  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_used.sort_values(by='recall score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "Zz7smHD9V3Xi",
    "outputId": "34fefb0c-ff20-436e-c532-c4654c8f2e91"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision score</th>\n",
       "      <th>recall score</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>difference in accuracy (in %)</th>\n",
       "      <th>difference in recall (in %)</th>\n",
       "      <th>difference in precision (in %)</th>\n",
       "      <th>difference in f1 score (in %)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic regression (under sampling)</td>\n",
       "      <td>0.942230</td>\n",
       "      <td>0.161976</td>\n",
       "      <td>0.883959</td>\n",
       "      <td>0.273784</td>\n",
       "      <td>-2.793142</td>\n",
       "      <td>-0.653969</td>\n",
       "      <td>78.531292</td>\n",
       "      <td>63.723221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>knn (under sampling)</td>\n",
       "      <td>0.952413</td>\n",
       "      <td>0.192070</td>\n",
       "      <td>0.892833</td>\n",
       "      <td>0.316133</td>\n",
       "      <td>-1.326739</td>\n",
       "      <td>1.858833</td>\n",
       "      <td>77.285475</td>\n",
       "      <td>62.127740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic regression (smote)</td>\n",
       "      <td>0.949831</td>\n",
       "      <td>0.180735</td>\n",
       "      <td>0.869625</td>\n",
       "      <td>0.299272</td>\n",
       "      <td>-2.887710</td>\n",
       "      <td>2.010107</td>\n",
       "      <td>76.826262</td>\n",
       "      <td>61.913445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>random forest (under sampling)</td>\n",
       "      <td>0.963336</td>\n",
       "      <td>0.249957</td>\n",
       "      <td>0.987713</td>\n",
       "      <td>0.398952</td>\n",
       "      <td>3.666361</td>\n",
       "      <td>1.228669</td>\n",
       "      <td>75.004319</td>\n",
       "      <td>60.104770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic regression (over sampling)</td>\n",
       "      <td>0.952178</td>\n",
       "      <td>0.188918</td>\n",
       "      <td>0.875085</td>\n",
       "      <td>0.310750</td>\n",
       "      <td>-3.887958</td>\n",
       "      <td>-0.249176</td>\n",
       "      <td>76.100751</td>\n",
       "      <td>59.886884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>decision tree (under sampling)</td>\n",
       "      <td>0.965010</td>\n",
       "      <td>0.257292</td>\n",
       "      <td>0.975427</td>\n",
       "      <td>0.407181</td>\n",
       "      <td>3.499020</td>\n",
       "      <td>2.457338</td>\n",
       "      <td>74.270796</td>\n",
       "      <td>59.281949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>knn (smote)</td>\n",
       "      <td>0.975740</td>\n",
       "      <td>0.316821</td>\n",
       "      <td>0.838225</td>\n",
       "      <td>0.459839</td>\n",
       "      <td>1.512920</td>\n",
       "      <td>16.095974</td>\n",
       "      <td>66.601695</td>\n",
       "      <td>53.110531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>knn (over sampling)</td>\n",
       "      <td>0.987664</td>\n",
       "      <td>0.499547</td>\n",
       "      <td>0.752901</td>\n",
       "      <td>0.600599</td>\n",
       "      <td>0.897717</td>\n",
       "      <td>24.709898</td>\n",
       "      <td>49.377981</td>\n",
       "      <td>39.605332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>decision tree (smote)</td>\n",
       "      <td>0.990464</td>\n",
       "      <td>0.581008</td>\n",
       "      <td>0.810239</td>\n",
       "      <td>0.676739</td>\n",
       "      <td>0.953590</td>\n",
       "      <td>18.976109</td>\n",
       "      <td>41.899168</td>\n",
       "      <td>32.326112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>random forest (smote)</td>\n",
       "      <td>0.991700</td>\n",
       "      <td>0.616585</td>\n",
       "      <td>0.862799</td>\n",
       "      <td>0.719203</td>\n",
       "      <td>0.829870</td>\n",
       "      <td>13.719924</td>\n",
       "      <td>38.341463</td>\n",
       "      <td>28.079552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>decision tree (over sampling)</td>\n",
       "      <td>0.993988</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.737201</td>\n",
       "      <td>0.751304</td>\n",
       "      <td>0.601250</td>\n",
       "      <td>26.279863</td>\n",
       "      <td>23.404255</td>\n",
       "      <td>24.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>decision tree (simple)</td>\n",
       "      <td>0.993945</td>\n",
       "      <td>0.747508</td>\n",
       "      <td>0.767918</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.605454</td>\n",
       "      <td>23.208191</td>\n",
       "      <td>25.249169</td>\n",
       "      <td>24.242424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>random forest (over sampling)</td>\n",
       "      <td>0.995207</td>\n",
       "      <td>0.812718</td>\n",
       "      <td>0.793857</td>\n",
       "      <td>0.803177</td>\n",
       "      <td>0.479318</td>\n",
       "      <td>20.614334</td>\n",
       "      <td>18.728162</td>\n",
       "      <td>19.682320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>random forest (simple)</td>\n",
       "      <td>0.995703</td>\n",
       "      <td>0.873824</td>\n",
       "      <td>0.761092</td>\n",
       "      <td>0.813572</td>\n",
       "      <td>0.429284</td>\n",
       "      <td>23.855911</td>\n",
       "      <td>12.617555</td>\n",
       "      <td>18.625391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>knn (simple)</td>\n",
       "      <td>0.993601</td>\n",
       "      <td>0.822936</td>\n",
       "      <td>0.612287</td>\n",
       "      <td>0.702153</td>\n",
       "      <td>0.113728</td>\n",
       "      <td>4.542909</td>\n",
       "      <td>5.183231</td>\n",
       "      <td>4.871826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Gradient boosting</td>\n",
       "      <td>0.995484</td>\n",
       "      <td>0.911348</td>\n",
       "      <td>0.701706</td>\n",
       "      <td>0.792904</td>\n",
       "      <td>0.026065</td>\n",
       "      <td>0.901714</td>\n",
       "      <td>0.646379</td>\n",
       "      <td>0.819666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic regression (simple)</td>\n",
       "      <td>0.993811</td>\n",
       "      <td>0.877720</td>\n",
       "      <td>0.578157</td>\n",
       "      <td>0.697119</td>\n",
       "      <td>0.013449</td>\n",
       "      <td>0.022138</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.016887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    name  accuracy  precision score  \\\n",
       "1   logistic regression (under sampling)  0.942230         0.161976   \n",
       "21                  knn (under sampling)  0.952413         0.192070   \n",
       "3            logistic regression (smote)  0.949831         0.180735   \n",
       "13        random forest (under sampling)  0.963336         0.249957   \n",
       "2    logistic regression (over sampling)  0.952178         0.188918   \n",
       "9         decision tree (under sampling)  0.965010         0.257292   \n",
       "23                           knn (smote)  0.975740         0.316821   \n",
       "22                   knn (over sampling)  0.987664         0.499547   \n",
       "11                 decision tree (smote)  0.990464         0.581008   \n",
       "15                 random forest (smote)  0.991700         0.616585   \n",
       "10         decision tree (over sampling)  0.993988         0.765957   \n",
       "8                 decision tree (simple)  0.993945         0.747508   \n",
       "14         random forest (over sampling)  0.995207         0.812718   \n",
       "12                random forest (simple)  0.995703         0.873824   \n",
       "20                          knn (simple)  0.993601         0.822936   \n",
       "24                     Gradient boosting  0.995484         0.911348   \n",
       "0           logistic regression (simple)  0.993811         0.877720   \n",
       "\n",
       "    recall score  f1 score  difference in accuracy (in %)  \\\n",
       "1       0.883959  0.273784                      -2.793142   \n",
       "21      0.892833  0.316133                      -1.326739   \n",
       "3       0.869625  0.299272                      -2.887710   \n",
       "13      0.987713  0.398952                       3.666361   \n",
       "2       0.875085  0.310750                      -3.887958   \n",
       "9       0.975427  0.407181                       3.499020   \n",
       "23      0.838225  0.459839                       1.512920   \n",
       "22      0.752901  0.600599                       0.897717   \n",
       "11      0.810239  0.676739                       0.953590   \n",
       "15      0.862799  0.719203                       0.829870   \n",
       "10      0.737201  0.751304                       0.601250   \n",
       "8       0.767918  0.757576                       0.605454   \n",
       "14      0.793857  0.803177                       0.479318   \n",
       "12      0.761092  0.813572                       0.429284   \n",
       "20      0.612287  0.702153                       0.113728   \n",
       "24      0.701706  0.792904                       0.026065   \n",
       "0       0.578157  0.697119                       0.013449   \n",
       "\n",
       "    difference in recall (in %)  difference in precision (in %)  \\\n",
       "1                     -0.653969                       78.531292   \n",
       "21                     1.858833                       77.285475   \n",
       "3                      2.010107                       76.826262   \n",
       "13                     1.228669                       75.004319   \n",
       "2                     -0.249176                       76.100751   \n",
       "9                      2.457338                       74.270796   \n",
       "23                    16.095974                       66.601695   \n",
       "22                    24.709898                       49.377981   \n",
       "11                    18.976109                       41.899168   \n",
       "15                    13.719924                       38.341463   \n",
       "10                    26.279863                       23.404255   \n",
       "8                     23.208191                       25.249169   \n",
       "14                    20.614334                       18.728162   \n",
       "12                    23.855911                       12.617555   \n",
       "20                     4.542909                        5.183231   \n",
       "24                     0.901714                        0.646379   \n",
       "0                      0.022138                        0.002523   \n",
       "\n",
       "    difference in f1 score (in %)  \n",
       "1                       63.723221  \n",
       "21                      62.127740  \n",
       "3                       61.913445  \n",
       "13                      60.104770  \n",
       "2                       59.886884  \n",
       "9                       59.281949  \n",
       "23                      53.110531  \n",
       "22                      39.605332  \n",
       "11                      32.326112  \n",
       "15                      28.079552  \n",
       "10                      24.869565  \n",
       "8                       24.242424  \n",
       "14                      19.682320  \n",
       "12                      18.625391  \n",
       "20                       4.871826  \n",
       "24                       0.819666  \n",
       "0                        0.016887  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_used.sort_values(by='difference in f1 score (in %)',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "prfe8Mj3WBsu",
    "outputId": "81c6e8aa-8211-43a7-9c2d-37214431cea2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision score</th>\n",
       "      <th>recall score</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>difference in accuracy (in %)</th>\n",
       "      <th>difference in recall (in %)</th>\n",
       "      <th>difference in precision (in %)</th>\n",
       "      <th>difference in f1 score (in %)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>decision tree (over sampling)</td>\n",
       "      <td>0.993988</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.737201</td>\n",
       "      <td>0.751304</td>\n",
       "      <td>0.601250</td>\n",
       "      <td>26.279863</td>\n",
       "      <td>23.404255</td>\n",
       "      <td>24.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>knn (over sampling)</td>\n",
       "      <td>0.987664</td>\n",
       "      <td>0.499547</td>\n",
       "      <td>0.752901</td>\n",
       "      <td>0.600599</td>\n",
       "      <td>0.897717</td>\n",
       "      <td>24.709898</td>\n",
       "      <td>49.377981</td>\n",
       "      <td>39.605332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>random forest (simple)</td>\n",
       "      <td>0.995703</td>\n",
       "      <td>0.873824</td>\n",
       "      <td>0.761092</td>\n",
       "      <td>0.813572</td>\n",
       "      <td>0.429284</td>\n",
       "      <td>23.855911</td>\n",
       "      <td>12.617555</td>\n",
       "      <td>18.625391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>decision tree (simple)</td>\n",
       "      <td>0.993945</td>\n",
       "      <td>0.747508</td>\n",
       "      <td>0.767918</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.605454</td>\n",
       "      <td>23.208191</td>\n",
       "      <td>25.249169</td>\n",
       "      <td>24.242424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>random forest (over sampling)</td>\n",
       "      <td>0.995207</td>\n",
       "      <td>0.812718</td>\n",
       "      <td>0.793857</td>\n",
       "      <td>0.803177</td>\n",
       "      <td>0.479318</td>\n",
       "      <td>20.614334</td>\n",
       "      <td>18.728162</td>\n",
       "      <td>19.682320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>decision tree (smote)</td>\n",
       "      <td>0.990464</td>\n",
       "      <td>0.581008</td>\n",
       "      <td>0.810239</td>\n",
       "      <td>0.676739</td>\n",
       "      <td>0.953590</td>\n",
       "      <td>18.976109</td>\n",
       "      <td>41.899168</td>\n",
       "      <td>32.326112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>knn (smote)</td>\n",
       "      <td>0.975740</td>\n",
       "      <td>0.316821</td>\n",
       "      <td>0.838225</td>\n",
       "      <td>0.459839</td>\n",
       "      <td>1.512920</td>\n",
       "      <td>16.095974</td>\n",
       "      <td>66.601695</td>\n",
       "      <td>53.110531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>random forest (smote)</td>\n",
       "      <td>0.991700</td>\n",
       "      <td>0.616585</td>\n",
       "      <td>0.862799</td>\n",
       "      <td>0.719203</td>\n",
       "      <td>0.829870</td>\n",
       "      <td>13.719924</td>\n",
       "      <td>38.341463</td>\n",
       "      <td>28.079552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>knn (simple)</td>\n",
       "      <td>0.993601</td>\n",
       "      <td>0.822936</td>\n",
       "      <td>0.612287</td>\n",
       "      <td>0.702153</td>\n",
       "      <td>0.113728</td>\n",
       "      <td>4.542909</td>\n",
       "      <td>5.183231</td>\n",
       "      <td>4.871826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>decision tree (under sampling)</td>\n",
       "      <td>0.965010</td>\n",
       "      <td>0.257292</td>\n",
       "      <td>0.975427</td>\n",
       "      <td>0.407181</td>\n",
       "      <td>3.499020</td>\n",
       "      <td>2.457338</td>\n",
       "      <td>74.270796</td>\n",
       "      <td>59.281949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic regression (smote)</td>\n",
       "      <td>0.949831</td>\n",
       "      <td>0.180735</td>\n",
       "      <td>0.869625</td>\n",
       "      <td>0.299272</td>\n",
       "      <td>-2.887710</td>\n",
       "      <td>2.010107</td>\n",
       "      <td>76.826262</td>\n",
       "      <td>61.913445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>knn (under sampling)</td>\n",
       "      <td>0.952413</td>\n",
       "      <td>0.192070</td>\n",
       "      <td>0.892833</td>\n",
       "      <td>0.316133</td>\n",
       "      <td>-1.326739</td>\n",
       "      <td>1.858833</td>\n",
       "      <td>77.285475</td>\n",
       "      <td>62.127740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>random forest (under sampling)</td>\n",
       "      <td>0.963336</td>\n",
       "      <td>0.249957</td>\n",
       "      <td>0.987713</td>\n",
       "      <td>0.398952</td>\n",
       "      <td>3.666361</td>\n",
       "      <td>1.228669</td>\n",
       "      <td>75.004319</td>\n",
       "      <td>60.104770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Gradient boosting</td>\n",
       "      <td>0.995484</td>\n",
       "      <td>0.911348</td>\n",
       "      <td>0.701706</td>\n",
       "      <td>0.792904</td>\n",
       "      <td>0.026065</td>\n",
       "      <td>0.901714</td>\n",
       "      <td>0.646379</td>\n",
       "      <td>0.819666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic regression (simple)</td>\n",
       "      <td>0.993811</td>\n",
       "      <td>0.877720</td>\n",
       "      <td>0.578157</td>\n",
       "      <td>0.697119</td>\n",
       "      <td>0.013449</td>\n",
       "      <td>0.022138</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.016887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic regression (over sampling)</td>\n",
       "      <td>0.952178</td>\n",
       "      <td>0.188918</td>\n",
       "      <td>0.875085</td>\n",
       "      <td>0.310750</td>\n",
       "      <td>-3.887958</td>\n",
       "      <td>-0.249176</td>\n",
       "      <td>76.100751</td>\n",
       "      <td>59.886884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic regression (under sampling)</td>\n",
       "      <td>0.942230</td>\n",
       "      <td>0.161976</td>\n",
       "      <td>0.883959</td>\n",
       "      <td>0.273784</td>\n",
       "      <td>-2.793142</td>\n",
       "      <td>-0.653969</td>\n",
       "      <td>78.531292</td>\n",
       "      <td>63.723221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    name  accuracy  precision score  \\\n",
       "10         decision tree (over sampling)  0.993988         0.765957   \n",
       "22                   knn (over sampling)  0.987664         0.499547   \n",
       "12                random forest (simple)  0.995703         0.873824   \n",
       "8                 decision tree (simple)  0.993945         0.747508   \n",
       "14         random forest (over sampling)  0.995207         0.812718   \n",
       "11                 decision tree (smote)  0.990464         0.581008   \n",
       "23                           knn (smote)  0.975740         0.316821   \n",
       "15                 random forest (smote)  0.991700         0.616585   \n",
       "20                          knn (simple)  0.993601         0.822936   \n",
       "9         decision tree (under sampling)  0.965010         0.257292   \n",
       "3            logistic regression (smote)  0.949831         0.180735   \n",
       "21                  knn (under sampling)  0.952413         0.192070   \n",
       "13        random forest (under sampling)  0.963336         0.249957   \n",
       "24                     Gradient boosting  0.995484         0.911348   \n",
       "0           logistic regression (simple)  0.993811         0.877720   \n",
       "2    logistic regression (over sampling)  0.952178         0.188918   \n",
       "1   logistic regression (under sampling)  0.942230         0.161976   \n",
       "\n",
       "    recall score  f1 score  difference in accuracy (in %)  \\\n",
       "10      0.737201  0.751304                       0.601250   \n",
       "22      0.752901  0.600599                       0.897717   \n",
       "12      0.761092  0.813572                       0.429284   \n",
       "8       0.767918  0.757576                       0.605454   \n",
       "14      0.793857  0.803177                       0.479318   \n",
       "11      0.810239  0.676739                       0.953590   \n",
       "23      0.838225  0.459839                       1.512920   \n",
       "15      0.862799  0.719203                       0.829870   \n",
       "20      0.612287  0.702153                       0.113728   \n",
       "9       0.975427  0.407181                       3.499020   \n",
       "3       0.869625  0.299272                      -2.887710   \n",
       "21      0.892833  0.316133                      -1.326739   \n",
       "13      0.987713  0.398952                       3.666361   \n",
       "24      0.701706  0.792904                       0.026065   \n",
       "0       0.578157  0.697119                       0.013449   \n",
       "2       0.875085  0.310750                      -3.887958   \n",
       "1       0.883959  0.273784                      -2.793142   \n",
       "\n",
       "    difference in recall (in %)  difference in precision (in %)  \\\n",
       "10                    26.279863                       23.404255   \n",
       "22                    24.709898                       49.377981   \n",
       "12                    23.855911                       12.617555   \n",
       "8                     23.208191                       25.249169   \n",
       "14                    20.614334                       18.728162   \n",
       "11                    18.976109                       41.899168   \n",
       "23                    16.095974                       66.601695   \n",
       "15                    13.719924                       38.341463   \n",
       "20                     4.542909                        5.183231   \n",
       "9                      2.457338                       74.270796   \n",
       "3                      2.010107                       76.826262   \n",
       "21                     1.858833                       77.285475   \n",
       "13                     1.228669                       75.004319   \n",
       "24                     0.901714                        0.646379   \n",
       "0                      0.022138                        0.002523   \n",
       "2                     -0.249176                       76.100751   \n",
       "1                     -0.653969                       78.531292   \n",
       "\n",
       "    difference in f1 score (in %)  \n",
       "10                      24.869565  \n",
       "22                      39.605332  \n",
       "12                      18.625391  \n",
       "8                       24.242424  \n",
       "14                      19.682320  \n",
       "11                      32.326112  \n",
       "23                      53.110531  \n",
       "15                      28.079552  \n",
       "20                       4.871826  \n",
       "9                       59.281949  \n",
       "3                       61.913445  \n",
       "21                      62.127740  \n",
       "13                      60.104770  \n",
       "24                       0.819666  \n",
       "0                        0.016887  \n",
       "2                       59.886884  \n",
       "1                       63.723221  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_used.sort_values(by='difference in recall (in %)',ascending=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bank_fraud models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
